

@article{Brassard2011,
abstract = {We describe two quantum algorithms to approximate the mean value of a black-box function. The first algorithm is novel and asymptotically optimal while the second is a variation on an earlier algorithm due to Aharonov. Both algorithms have their own strengths and caveats and may be relevant in different contexts. We then propose a new algorithm for approximating the median of a set of points over an arbitrary distance function.},
archivePrefix = {arXiv},
arxivId = {1106.4267},
author = {Brassard, Gilles and Dupuis, Frederic and Gambs, Sebastien and Tapp, Alain},
eprint = {1106.4267},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/1106.4267.pdf:pdf},
pages = {1--10},
title = {{An optimal quantum algorithm to approximate the mean and its application for approximating the median of a set of points over an arbitrary distance}},
url = {http://arxiv.org/abs/1106.4267},
year = {2011}
}
@article{Duan2020,
abstract = {The Harrow-Hassidim-Lloyd (HHL) algorithm is a method to solve the quantum linear system of equations that may be found at the core of various scientific applications and quantum machine learning models including the linear regression, support vector machines and recommender systems etc. After reviewing the necessary background on elementary quantum algorithms, we provide detailed account of how HHL is exploited in different quantum machine learning (QML) models, and how it provides the desired quantum speedup in all these models. At the end, we briefly discuss some of the remaining challenges ahead for HHL-based QML models and related methods.},
author = {Duan, Bojia and Yuan, Jiabin and Yu, Chao Hua and Huang, Jianbang and Hsieh, Chang Yu},
doi = {10.1016/j.physleta.2020.126595},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/1-s2.0-S037596012030462X-main.pdf:pdf},
issn = {03759601},
journal = {Physics Letters, Section A: General, Atomic and Solid State Physics},
keywords = {HHL algorithm,Quantum circuit,Quantum computation,Quantum machine learning},
number = {24},
pages = {126595},
publisher = {Elsevier B.V.},
title = {{A survey on HHL algorithm: From theory to application in quantum machine learning}},
url = {https://doi.org/10.1016/j.physleta.2020.126595},
volume = {384},
year = {2020}
}
@article{Law,
abstract = {The Law of Large numbers Suppose we perform an experiment and a measurement encoded in the random variable X and that we repeat this experiment n times each time in the same conditions and each time independently of each other. We thus obtain n independent copies of the random variable X which we denote X 1 , X 2 , {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} , X n Such a collection of random variable is called a IID sequence of random variables where IID stands for independent and identically distributed. This means that the random variables X i have the same probability distribution. In particular they have all the same means and variance E[X i ] = µ , var(X i) = $\sigma$ 2 , i = 1, 2, {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} , n Each time we perform the experiment n tiimes, the X i provides a (random) measurement and if the average value X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} + X n n is called the empirical average. The Law of Large Numbers states for large n the empirical average is very close to the expected value µ with very high probability Theorem 1. Let X 1 , {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} , X n IID random variables with E[X i ] = µ and var(X i) for all i. Then we have P X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} X n n − µ ≥ ≤ $\sigma$ 2 nn 2 In particular the right hand side goes to 0 has n → ∞. Proof. The proof of the law of large numbers is a simple application from Chebyshev inequality to the random variable X 1 +{\textperiodcentered}{\textperiodcentered}{\textperiodcentered}Xn n. Indeed by the properties of expectations we have E X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} X n n = 1 n E [X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} X n ] = 1 n (E [X 1 ] + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} E [X n ]) = 1 n nµ = µ For the variance we use that the X i are independent and so we have var X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} X n n = 1 n 2 var (X 1 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} X n ]) = 1 n 2 (var(X 1) + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} + var(X n)) = $\sigma$ 2 n 1},
author = {Law, The and Law, The and Numbers, Large},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/lecture17.pdf:pdf},
pages = {1--2},
title = {{Lecture 17: The Law of Large Numbers and the Monte-Carlo method}},
url = {http://people.math.umass.edu/$\sim$lr7q/ps_files/teaching/math456/lecture17.pdf}
}
@article{MonteCarlo_notes,
abstract = {We start our study of Monte Carlo methods with what is usually called direct or simple Monte Carlo. We will refer to it as direct Monte Carlo. We assume that our original problem can be put in the following form. There is a probability space (Ω, P) and a random variable X on it. The quantity we want to compute is the the mean of X which we denote by µ = E[X]. (The sample space Ω is the set of possible outcomes. Subsets of Ω are called events, and the probability measure P a function that assigns a number between 0 and 1 to each event. Usually the probability measure is only defined on a $\sigma$-field F, which is a sub-collection of the subsets of Ω, but we will not worry about this.) We emphasize that the original problem need not involve any randomness, even if it does the probability space we use for the Monte Carlo may not have been part of the original problem. Let X n be an independent, identically distributed (iid) sequence which has the same distribution as X. Recall that saying X n and X are indentically distributed means that for all Borel sets B, P (X n ∈ B) = P (X ∈ B). A standard result in probability says that if they are equal for all B whic are intervals, then that is sufficient to insure they are equal for all Borel sets. The key theorem that underlies direct Monte Carlo is the Strong Law of Large Numbers. Theorem 1 Let X n be an iid sequence such that E[|X n |] < ∞. Let µ = E[X n ]. Then P (lim n→∞ 1 n n k=1 X k = µ) = 1 (2.1) 1},
author = {MonteCarlo notes},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/book_chap2.pdf:pdf},
title = {{Chapter 2 Basics of direct Monte Carlo 2.1 The probabilistic basis for direct MC}},
url = {https://math.arizona.edu/$\sim$tgk/mc/book_chap2.pdf},
volume = {1}
}
@article{Brassard2002,
abstract = {Consider a Boolean function $\chi: X \to \{0,1\}$ that partitions set $X$ between its good and bad elements, where $x$ is good if $\chi(x)=1$ and bad otherwise. Consider also a quantum algorithm $\mathcal A$ such that $A |0\rangle= \sum_{x\in X} \alpha_x |x\rangle$ is a quantum superposition of the elements of $X$, and let $a$ denote the probability that a good element is produced if $A |0\rangle$ is measured. If we repeat the process of running $A$, measuring the output, and using $\chi$ to check the validity of the result, we shall expect to repeat $1/a$ times on the average before a solution is found. *Amplitude amplification* is a process that allows to find a good $x$ after an expected number of applications of $A$ and its inverse which is proportional to $1/\sqrt{a}$, assuming algorithm $A$ makes no measurements. This is a generalization of Grover's searching algorithm in which $A$ was restricted to producing an equal superposition of all members of $X$ and we had a promise that a single $x$ existed such that $\chi(x)=1$. Our algorithm works whether or not the value of $a$ is known ahead of time. In case the value of $a$ is known, we can find a good $x$ after a number of applications of $A$ and its inverse which is proportional to $1/\sqrt{a}$ even in the worst case. We show that this quadratic speedup can also be obtained for a large family of search problems for which good classical heuristics exist. Finally, as our main result, we combine ideas from Grover's and Shor's quantum algorithms to perform amplitude estimation, a process that allows to estimate the value of $a$. We apply amplitude estimation to the problem of *approximate counting*, in which we wish to estimate the number of $x\in X$ such that $\chi(x)=1$. We obtain optimal quantum algorithms in a variety of settings.},
archivePrefix = {arXiv},
arxivId = {quant-ph/0005055},
author = {Brassard, Gilles and H{\o}yer, Peter and Mosca, Michele and Tapp, Alain},
doi = {10.1090/conm/305/05215},
eprint = {0005055},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/{\`{e}}{\S}°{\c{c}}+.pdf:pdf},
pages = {53--74},
primaryClass = {quant-ph},
title = {{Quantum amplitude amplification and estimation}},
year = {2002}
}
@article{Wocjan2009,
abstract = {We present a quantum algorithm based on classical fully polynomial randomized approximation schemes (FPRASs) for estimating partition functions that combine simulated annealing with the Monte Carlo Markov chain method and use nonadaptive cooling schedules. We achieve a twofold polynomial improvement in time complexity: a quadratic reduction with respect to the spectral gap of the underlying Markov chains and a quadratic reduction with respect to the parameter characterizing the desired accuracy of the estimate output by the FPRAS. Both reductions are intimately related and cannot be achieved separately. First, we use Grover's fixed-point search, quantum walks, and phase estimation to efficiently prepare approximate coherent encodings of stationary distributions of the Markov chains. The speed up we obtain in this way is due to the quadratic relation between the spectral and phase gaps of classical and quantum walks. The second speed up with respect to accuracy comes from generalized quantum counting used instead of classical sampling to estimate expected values of quantum observables. {\textcopyright} 2009 The American Physical Society.},
archivePrefix = {arXiv},
arxivId = {arXiv:0811.0596v3},
author = {Wocjan, Pawel and Chiang, Chen Fu and Nagaj, Daniel and Abeyesinghe, Anura},
doi = {10.1103/PhysRevA.80.022340},
eprint = {arXiv:0811.0596v3},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/0811.0596.pdf:pdf},
issn = {10502947},
journal = {Physical Review A - Atomic, Molecular, and Optical Physics},
number = {2},
pages = {1--18},
title = {{Quantum algorithm for approximating partition functions}},
volume = {80},
year = {2009}
}
@article{DeWolf2019,
abstract = {This is a set of lecture notes suitable for a Master's course on quantum computation and information from the perspective of theoretical computer science. The first version was written in 2011, with many extensions and improvements in subsequent years. The first 10 chapters cover the circuit model and the main quantum algorithms (Deutsch-Jozsa, Simon, Shor, Hidden Subgroup Problem, Grover, quantum walks, Hamiltonian simulation and HHL). They are followed by 3 chapters about complexity, 4 chapters about distributed ("Alice and Bob") settings, and a final chapter about quantum error correction. Appendices A and B give a brief introduction to the required linear algebra and some other mathematical and computer science background. All chapters come with exercises, with some hints provided in Appendix C.},
archivePrefix = {arXiv},
arxivId = {1907.09415},
author = {de Wolf, Ronald},
eprint = {1907.09415},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/1907.09415.pdf:pdf},
title = {{Quantum Computing: Lecture Notes}},
url = {http://arxiv.org/abs/1907.09415},
year = {2019}
}
@article{Heinrich2002,
abstract = {We study summation of sequences and integration in the quantum model of computation. We develop quantum algorithms for computing the mean of sequences that satisfy a p-summability condition and for integration of functions from Lebesgue spaces Lp([0,1]d), and analyze their convergence rates. We also prove lower bounds showing that the proposed algorithms are, in many cases, optimal within the setting of quantum computing. This extends recent results of G. Brassard et al. (2000, "Quantum Amplitude Amplification and Estimation," Technical Report, http://arXiv.org/abs/quant-ph/0005055) on computing the mean for bounded sequences and complements results of E. Novak (2001, J. Complexity 17, 2-16) on integration of functions from H{\"{o}}lder classes. The analysis requires an appropriate model of quantum computation, capable of covering the typical features of numerical problems such as dealing with real numbers and real-valued functions and with vector and function spaces. We develop and study such a model, which can be viewed as a quantum setting for information-based complexity theory. {\textcopyright} 2002 Elsevier Science (USA).},
archivePrefix = {arXiv},
arxivId = {quant-ph/0105116},
author = {Heinrich, S.},
doi = {10.1006/jcom.2001.0629},
eprint = {0105116},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/0105116.pdf:pdf},
issn = {0885064X},
journal = {Journal of Complexity},
number = {1},
pages = {1--50},
primaryClass = {quant-ph},
title = {{Quantum summation with an application to integration}},
volume = {18},
year = {2002}
}
@book{Rieffel2000,
abstract = {Richard Feynman's observation that certain quantum mechanical effects cannot be simulated efficiently on a computer led to speculation that computation in general could be done more efficiently if it used these quantum effects. This speculation proved justified when Peter Shor described a polynomial time quantum algorithm for factoring integers. In quantum systems, the computational space increases exponentially with the size of the system, which enables exponential parallelism. This parallelism could lead to exponentially faster quantum algorithms than possible classically. The catch is that accessing the results, which requires measurement, proves tricky and requires new nontraditional programming techniques. The aim of this paper is to guide computer scientists through the barriers that separate quantum computing from conventional computing. We introduce basic principles of quantum mechanics to explain where the power of quantum computers comes from and why it is difficult to harness. We describe quantum cryptography, teleportation, and dense coding. Various approaches to exploiting the power of quantum parallelism are explained. We conclude with a discussion of quantum error correction. Categories and Subject Descriptors: A.I [Introductory and Survey] General Terms: Algorithms, Security, Theory. {\textcopyright} 2001 ACM.},
archivePrefix = {arXiv},
arxivId = {quant-ph/9809016},
author = {Rieffel, Eleanor and Polak, Wolfgang},
booktitle = {ACM Computing Surveys},
doi = {10.1145/367701.367709},
eprint = {9809016},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/W020170224608149125645.pdf:pdf},
isbn = {9780198570004},
issn = {03600300},
keywords = {Complexity,Parallelism,Quantum computing},
number = {3},
pages = {300--335},
primaryClass = {quant-ph},
title = {{An introduction to quantum computing for non-physicists}},
volume = {32},
year = {2000}
}
@article{Montanaro2015,
abstract = {Monte Carlo methods use random sampling to estimate numerical quantities which are hard to compute deterministically. One important example is the use in statistical physics of rapidly mixing Markov chains to approximately compute partition functions. In this work, we describe a quantum algorithm which can accelerate Monte Carlo methods in a very general setting. The algorithm estimates the expected output value of an arbitrary randomized or quantum subroutine with bounded variance, achieving a near-quadratic speedup over the best possible classical algorithm. Combining the algorithm with the use of quantum walks gives a quantum speedup of the fastest known classical algorithms with rigorous performance bounds for computing partition functions, which use multiple-stage Markov chain Monte Carlo techniques. The quantum algorithm can also be used to estimate the total variation distance between probability distributions efficiently.},
archivePrefix = {arXiv},
arxivId = {1504.06987},
author = {Montanaro, Ashley},
doi = {10.1098/rspa.2015.0301},
eprint = {1504.06987},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/1504.06987.pdf:pdf},
issn = {14712946},
journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
keywords = {Monte Carlo methods,Partition functions,Quantum algorithms},
number = {2181},
pages = {1--28},
title = {{Quantum speedup of Monte Carlo methods}},
volume = {471},
year = {2015}
}
@article{Lapeyre2007,
author = {Lapeyre, Bernard},
file = {:Users/michelevischi/Desktop/MIE COSE/QOSF/lecture-1.pdf:pdf},
number = {January},
pages = {1--10},
title = {{Introduction to Monte-Carlo Methods 1 On the convergence rate of Monte-Carlo methods}},
year = {2007}
}
