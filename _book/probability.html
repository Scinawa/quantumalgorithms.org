<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>C Probability | Quantum algorithms for data analysis</title>
  <meta name="description" content="Open-source book on quantum algorithms for information processing and machine learning" />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="C Probability | Quantum algorithms for data analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Open-source book on quantum algorithms for information processing and machine learning" />
  <meta name="github-repo" content="scinawa/quantumalgorithms.org" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="C Probability | Quantum algorithms for data analysis" />
  
  <meta name="twitter:description" content="Open-source book on quantum algorithms for information processing and machine learning" />
  

<meta name="author" content="Alessandro Luongo" />


<meta name="date" content="2022-06-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="series.html"/>
<link rel="next" href="appendix-contributors.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3SFGGERDM4"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3SFGGERDM4');
</script>
<link rel="shortcut icon" href="favicon/favicon.ico" />



<!-- Google Tag Manager -->
<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
})(window,document,'script','dataLayer','GTM-W4MVX5C');</script>
<!-- End Google Tag Manager -->



<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Quantum algorithms for data analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Preface</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#abstract"><i class="fa fa-check"></i><b>1.1</b> Abstract</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#changelog"><i class="fa fa-check"></i><b>1.2</b> Changelog</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#teaching-using-this-book"><i class="fa fa-check"></i><b>1.3</b> Teaching using this book</a></li>
</ul></li>
<li class="part"><span><b>I Bridging the gap</b></span></li>
<li class="chapter" data-level="2" data-path="chapter-intro.html"><a href="chapter-intro.html"><i class="fa fa-check"></i><b>2</b> Quantum computing and quantum algorithms</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter-intro.html"><a href="chapter-intro.html#getting-rid-of-physics-in-quantum-computing"><i class="fa fa-check"></i><b>2.1</b> Getting rid of physics in quantum computing</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-intro.html"><a href="chapter-intro.html#section-axioms"><i class="fa fa-check"></i><b>2.2</b> Axioms of quantum mechanics</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter-intro.html"><a href="chapter-intro.html#review-of-important-statements-in-quantum-computation"><i class="fa fa-check"></i><b>2.2.1</b> Review of important statements in quantum computation</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chapter-intro.html"><a href="chapter-intro.html#measuring-complexity"><i class="fa fa-check"></i><b>2.3</b> Measuring complexity of quantum algorithms</a></li>
<li class="chapter" data-level="2.4" data-path="chapter-intro.html"><a href="chapter-intro.html#review-famous-quantum-algos"><i class="fa fa-check"></i><b>2.4</b> Review of famous quantum algorithms</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="chapter-intro.html"><a href="chapter-intro.html#deutsch-josza"><i class="fa fa-check"></i><b>2.4.1</b> Deutsch-Josza</a></li>
<li class="chapter" data-level="2.4.2" data-path="chapter-intro.html"><a href="chapter-intro.html#bernstein-vazirani"><i class="fa fa-check"></i><b>2.4.2</b> Bernstein-Vazirani</a></li>
<li class="chapter" data-level="2.4.3" data-path="chapter-intro.html"><a href="chapter-intro.html#hadamard-test"><i class="fa fa-check"></i><b>2.4.3</b> Hadamard test</a></li>
<li class="chapter" data-level="2.4.4" data-path="chapter-intro.html"><a href="chapter-intro.html#modified-hadamard-test"><i class="fa fa-check"></i><b>2.4.4</b> Modified Hadamard test</a></li>
<li class="chapter" data-level="2.4.5" data-path="chapter-intro.html"><a href="chapter-intro.html#swap-test"><i class="fa fa-check"></i><b>2.4.5</b> Swap test</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html"><i class="fa fa-check"></i><b>3</b> Classical data in quantum computers</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#representing-data-in-quantum-computers"><i class="fa fa-check"></i><b>3.1</b> Representing data in quantum computers</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#sec:numbers"><i class="fa fa-check"></i><b>3.1.1</b> Numbers and quantum arithmetics</a></li>
<li class="chapter" data-level="3.1.2" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#subsec-stateprep-matrices"><i class="fa fa-check"></i><b>3.1.2</b> Vectors and matrices</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#sec:quantum-memory-models"><i class="fa fa-check"></i><b>3.2</b> Access models</a></li>
<li class="chapter" data-level="3.3" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#implementations"><i class="fa fa-check"></i><b>3.3</b> Implementations</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#subsec:qram-model"><i class="fa fa-check"></i><b>3.3.1</b> Quantum memory models and the QRAM</a></li>
<li class="chapter" data-level="3.3.2" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#sec:accessmodel-circuits"><i class="fa fa-check"></i><b>3.3.2</b> Circuits</a></li>
<li class="chapter" data-level="3.3.3" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#q-sampling-access"><i class="fa fa-check"></i><b>3.3.3</b> Quantum sampling access</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#block-encodings"><i class="fa fa-check"></i><b>3.4</b> Block encodings</a></li>
<li class="chapter" data-level="3.5" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#importance-of-quantum-memory-models"><i class="fa fa-check"></i><b>3.5</b> Importance of quantum memory models</a></li>
<li class="chapter" data-level="3.6" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#sec:qramarchitectures"><i class="fa fa-check"></i><b>3.6</b> QRAM architecures and noise resilience</a></li>
<li class="chapter" data-level="3.7" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#working-with-classical-probability-distributions"><i class="fa fa-check"></i><b>3.7</b> Working with classical probability distributions</a></li>
<li class="chapter" data-level="3.8" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#retrieving-data"><i class="fa fa-check"></i><b>3.8</b> Retrieving data</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="chap-classical-data-quantum-computers.html"><a href="chap-classical-data-quantum-computers.html#denisty-matrices"><i class="fa fa-check"></i><b>3.8.1</b> Denisty matrices</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html"><i class="fa fa-check"></i><b>4</b> Classical machine learning</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#supervised-learning"><i class="fa fa-check"></i><b>4.1</b> Supervised learning</a></li>
<li class="chapter" data-level="4.2" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.2</b> Unsupervised learning</a></li>
<li class="chapter" data-level="4.3" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#generative-and-discriminative-models"><i class="fa fa-check"></i><b>4.3</b> Generative and discriminative models</a></li>
<li class="chapter" data-level="4.4" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#dimensionality-reduction"><i class="fa fa-check"></i><b>4.4</b> Dimensionality reduction</a></li>
<li class="chapter" data-level="4.5" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#generalized-eigenvalue-problems-in-machine-learning"><i class="fa fa-check"></i><b>4.5</b> Generalized eigenvalue problems in machine learning</a></li>
<li class="chapter" data-level="4.6" data-path="chap-machinelearning.html"><a href="chap-machinelearning.html#how-to-evaluate-a-classifier"><i class="fa fa-check"></i><b>4.6</b> How to evaluate a classifier</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chap-toolbox.html"><a href="chap-toolbox.html"><i class="fa fa-check"></i><b>5</b> A useful toolbox</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chap-toolbox.html"><a href="chap-toolbox.html#section:phaseestimation"><i class="fa fa-check"></i><b>5.1</b> Phase estimation</a></li>
<li class="chapter" data-level="5.2" data-path="chap-toolbox.html"><a href="chap-toolbox.html#section:grover"><i class="fa fa-check"></i><b>5.2</b> Grover’s algorithm, amplitude games</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="chap-toolbox.html"><a href="chap-toolbox.html#example-estimating-average-and-variance-of-a-function"><i class="fa fa-check"></i><b>5.2.1</b> Example: estimating average and variance of a function</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="chap-toolbox.html"><a href="chap-toolbox.html#subsec:findmin"><i class="fa fa-check"></i><b>5.3</b> Finding the minimum</a></li>
<li class="chapter" data-level="5.4" data-path="chap-toolbox.html"><a href="chap-toolbox.html#subsec:linearalgebra"><i class="fa fa-check"></i><b>5.4</b> Quantum linear algebra</a></li>
<li class="chapter" data-level="5.5" data-path="chap-toolbox.html"><a href="chap-toolbox.html#linear-combination-of-unitaries"><i class="fa fa-check"></i><b>5.5</b> Linear combination of unitaries</a></li>
<li class="chapter" data-level="5.6" data-path="chap-toolbox.html"><a href="chap-toolbox.html#subsec:svt"><i class="fa fa-check"></i><b>5.6</b> Singular value transformation</a></li>
<li class="chapter" data-level="5.7" data-path="chap-toolbox.html"><a href="chap-toolbox.html#sub:distances"><i class="fa fa-check"></i><b>5.7</b> Distances, inner products, norms, and quadratic forms</a>
<ul>
<li class="chapter" data-level="5.7.1" data-path="chap-toolbox.html"><a href="chap-toolbox.html#inner-products-and-quadratic-forms-with-kp-trees"><i class="fa fa-check"></i><b>5.7.1</b> Inner products and quadratic forms with KP-trees</a></li>
<li class="chapter" data-level="5.7.2" data-path="chap-toolbox.html"><a href="chap-toolbox.html#inner-product-and-l1-norm-estimation-with-query-access"><i class="fa fa-check"></i><b>5.7.2</b> Inner product and l1-norm estimation with query access</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="chap-toolbox.html"><a href="chap-toolbox.html#hamiltonian-simulation"><i class="fa fa-check"></i><b>5.8</b> Hamiltonian simulation</a>
<ul>
<li class="chapter" data-level="5.8.1" data-path="chap-toolbox.html"><a href="chap-toolbox.html#introduction-to-hamiltonians"><i class="fa fa-check"></i><b>5.8.1</b> Introduction to Hamiltonians</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>II Quantum Machine Learning</b></span></li>
<li class="chapter" data-level="6" data-path="quantum-perceptron.html"><a href="quantum-perceptron.html"><i class="fa fa-check"></i><b>6</b> Quantum perceptron</a>
<ul>
<li class="chapter" data-level="6.1" data-path="quantum-perceptron.html"><a href="quantum-perceptron.html#classical-perceptron"><i class="fa fa-check"></i><b>6.1</b> Classical perceptron</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="quantum-perceptron.html"><a href="quantum-perceptron.html#training-the-perceptron"><i class="fa fa-check"></i><b>6.1.1</b> Training the perceptron</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="quantum-perceptron.html"><a href="quantum-perceptron.html#online-quantum-perceptron"><i class="fa fa-check"></i><b>6.2</b> Online quantum perceptron</a></li>
<li class="chapter" data-level="6.3" data-path="quantum-perceptron.html"><a href="quantum-perceptron.html#version-space-quantum-perceptron"><i class="fa fa-check"></i><b>6.3</b> Version space quantum perceptron</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chap-svebased.html"><a href="chap-svebased.html"><i class="fa fa-check"></i><b>7</b> SVE-based quantum algorithms</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chap-svebased.html"><a href="chap-svebased.html#estimation-of-the-spectral-norm-and-the-condition-number"><i class="fa fa-check"></i><b>7.1</b> Estimation of the spectral norm and the condition number</a></li>
<li class="chapter" data-level="7.2" data-path="chap-svebased.html"><a href="chap-svebased.html#sec-explainedvariance"><i class="fa fa-check"></i><b>7.2</b> Explained variance</a></li>
<li class="chapter" data-level="7.3" data-path="chap-svebased.html"><a href="chap-svebased.html#singular-value-estimation-of-a-product-of-two-matrices"><i class="fa fa-check"></i><b>7.3</b> Singular value estimation of a product of two matrices</a></li>
<li class="chapter" data-level="7.4" data-path="chap-svebased.html"><a href="chap-svebased.html#log-determinant"><i class="fa fa-check"></i><b>7.4</b> Log-determinant</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html"><i class="fa fa-check"></i><b>8</b> Quantum algorithms for Monte Carlo</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#monte-carlo-with-quantum-computing"><i class="fa fa-check"></i><b>8.1</b> Monte Carlo with quantum computing</a></li>
<li class="chapter" data-level="8.2" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#bounded-output"><i class="fa fa-check"></i><b>8.2</b> Bounded output</a></li>
<li class="chapter" data-level="8.3" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#bounded-ell_2-norm"><i class="fa fa-check"></i><b>8.3</b> Bounded <span class="math inline">\(\ell_2\)</span> norm</a></li>
<li class="chapter" data-level="8.4" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#bounded-variance"><i class="fa fa-check"></i><b>8.4</b> Bounded variance</a></li>
<li class="chapter" data-level="8.5" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#applications"><i class="fa fa-check"></i><b>8.5</b> Applications</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="chap-montecarlo.html"><a href="chap-montecarlo.html#pricing-of-financial-derivatives"><i class="fa fa-check"></i><b>8.5.1</b> Pricing of financial derivatives</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html"><i class="fa fa-check"></i><b>9</b> Dimensionality reduction</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#unsupervised-algorithms"><i class="fa fa-check"></i><b>9.1</b> Unsupervised algorithms</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#sec:qpca"><i class="fa fa-check"></i><b>9.1.1</b> Quantum PCA</a></li>
<li class="chapter" data-level="9.1.2" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#section:qca"><i class="fa fa-check"></i><b>9.1.2</b> Quantum Correspondence Analysis</a></li>
<li class="chapter" data-level="9.1.3" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#section:qlsa"><i class="fa fa-check"></i><b>9.1.3</b> Quantum Latent Semantic Analysis</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#supervised-algorithms"><i class="fa fa-check"></i><b>9.2</b> Supervised algorithms</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="chap-dimensionality-reduction.html"><a href="chap-dimensionality-reduction.html#section:qsfa"><i class="fa fa-check"></i><b>9.2.1</b> Quantum Slow Feature Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="10" data-path="chap-q-means.html"><a href="chap-q-means.html"><i class="fa fa-check"></i><b>10</b> q-means</a>
<ul>
<li class="chapter" data-level="10.1" data-path="chap-q-means.html"><a href="chap-q-means.html#the-k-means-algorithm"><i class="fa fa-check"></i><b>10.1</b> The k-means algorithm</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="chap-q-means.html"><a href="chap-q-means.html#delta-k-means"><i class="fa fa-check"></i><b>10.1.1</b> <span class="math inline">\(\delta-\)</span>k-means</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="chap-q-means.html"><a href="chap-q-means.html#the-q-means-algorithm"><i class="fa fa-check"></i><b>10.2</b> The <span class="math inline">\(q\)</span>-means algorithm</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="chap-q-means.html"><a href="chap-q-means.html#step-1-centroid-distance-estimation"><i class="fa fa-check"></i><b>10.2.1</b> Step 1: Centroid distance estimation</a></li>
<li class="chapter" data-level="10.2.2" data-path="chap-q-means.html"><a href="chap-q-means.html#step-2-cluster-assignment"><i class="fa fa-check"></i><b>10.2.2</b> Step 2: Cluster assignment</a></li>
<li class="chapter" data-level="10.2.3" data-path="chap-q-means.html"><a href="chap-q-means.html#step-3-centroid-state-creation"><i class="fa fa-check"></i><b>10.2.3</b> Step 3: Centroid state creation</a></li>
<li class="chapter" data-level="10.2.4" data-path="chap-q-means.html"><a href="chap-q-means.html#step-4-centroid-update"><i class="fa fa-check"></i><b>10.2.4</b> Step 4: Centroid update</a></li>
<li class="chapter" data-level="10.2.5" data-path="chap-q-means.html"><a href="chap-q-means.html#initialization-of-q-means"><i class="fa fa-check"></i><b>10.2.5</b> Initialization of <span class="math inline">\(q\)</span>-means++</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="chap-q-means.html"><a href="chap-q-means.html#section:q-means-analysis"><i class="fa fa-check"></i><b>10.3</b> Analysis</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="chap-q-means.html"><a href="chap-q-means.html#error-analysis"><i class="fa fa-check"></i><b>10.3.1</b> Error analysis</a></li>
<li class="chapter" data-level="10.3.2" data-path="chap-q-means.html"><a href="chap-q-means.html#runtime-analysis"><i class="fa fa-check"></i><b>10.3.2</b> Runtime analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="chap-qem.html"><a href="chap-qem.html"><i class="fa fa-check"></i><b>11</b> Quantum Expectation-Maximization</a>
<ul>
<li class="chapter" data-level="11.1" data-path="chap-qem.html"><a href="chap-qem.html#expectation-maximization-for-gmm"><i class="fa fa-check"></i><b>11.1</b> Expectation-Maximization for GMM</a></li>
<li class="chapter" data-level="11.2" data-path="chap-qem.html"><a href="chap-qem.html#expectation-maximization"><i class="fa fa-check"></i><b>11.2</b> Expectation-Maximization</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="chap-qem.html"><a href="chap-qem.html#sec:initialization"><i class="fa fa-check"></i><b>11.2.1</b> Initialization strategies for EM</a></li>
<li class="chapter" data-level="11.2.2" data-path="chap-qem.html"><a href="chap-qem.html#dataset-assumption"><i class="fa fa-check"></i><b>11.2.2</b> Dataset assumptions in GMM</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="chap-qem.html"><a href="chap-qem.html#section:quantum-gmm"><i class="fa fa-check"></i><b>11.3</b> Quantum Expectation-Maximization for GMM</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="chap-qem.html"><a href="chap-qem.html#expectation"><i class="fa fa-check"></i><b>11.3.1</b> Expectation</a></li>
<li class="chapter" data-level="11.3.2" data-path="chap-qem.html"><a href="chap-qem.html#maximization"><i class="fa fa-check"></i><b>11.3.2</b> Maximization</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html"><i class="fa fa-check"></i><b>12</b> QML on real datasets</a>
<ul>
<li class="chapter" data-level="12.1" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#theoretical-considerations"><i class="fa fa-check"></i><b>12.1</b> Theoretical considerations</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#qmeans-datasetassumption"><i class="fa fa-check"></i><b>12.1.1</b> Modelling well-clusterable datasets</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#experiments"><i class="fa fa-check"></i><b>12.2</b> Experiments</a>
<ul>
<li class="chapter" data-level="12.2.1" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#datasets"><i class="fa fa-check"></i><b>12.2.1</b> Datasets</a></li>
<li class="chapter" data-level="12.2.2" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#q-means"><i class="fa fa-check"></i><b>12.2.2</b> q-means</a></li>
<li class="chapter" data-level="12.2.3" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#qsfa"><i class="fa fa-check"></i><b>12.2.3</b> QSFA</a></li>
<li class="chapter" data-level="12.2.4" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#qem"><i class="fa fa-check"></i><b>12.2.4</b> QEM</a></li>
<li class="chapter" data-level="12.2.5" data-path="chap-QML-on-real-data.html"><a href="chap-QML-on-real-data.html#qpca"><i class="fa fa-check"></i><b>12.2.5</b> QPCA</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="chap-graphs.html"><a href="chap-graphs.html"><i class="fa fa-check"></i><b>13</b> Quantum algorithms for graph problems</a>
<ul>
<li class="chapter" data-level="13.1" data-path="chap-graphs.html"><a href="chap-graphs.html#connectivity"><i class="fa fa-check"></i><b>13.1</b> Connectivity</a></li>
<li class="chapter" data-level="13.2" data-path="chap-graphs.html"><a href="chap-graphs.html#summary-of-results"><i class="fa fa-check"></i><b>13.2</b> Summary of results</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="chap-lowerbounds.html"><a href="chap-lowerbounds.html"><i class="fa fa-check"></i><b>14</b> Lower bounds on query complexity of quantum algorithms</a>
<ul>
<li class="chapter" data-level="14.1" data-path="chap-lowerbounds.html"><a href="chap-lowerbounds.html#polynomial-method"><i class="fa fa-check"></i><b>14.1</b> Polynomial method</a></li>
<li class="chapter" data-level="14.2" data-path="chap-lowerbounds.html"><a href="chap-lowerbounds.html#quantum-adversary-method"><i class="fa fa-check"></i><b>14.2</b> Quantum adversary method</a></li>
</ul></li>
<li class="part"><span><b>III Everything else</b></span></li>
<li class="chapter" data-level="15" data-path="selected-works-on-quantum-algorithms.html"><a href="selected-works-on-quantum-algorithms.html"><i class="fa fa-check"></i><b>15</b> Selected works on quantum algorithms</a></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html"><i class="fa fa-check"></i><b>A</b> Math and linear algebra</a>
<ul>
<li class="chapter" data-level="A.1" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#norms-distances-trace-inequalities"><i class="fa fa-check"></i><b>A.1</b> Norms, distances, trace, inequalities</a></li>
<li class="chapter" data-level="A.2" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#linear-algebra"><i class="fa fa-check"></i><b>A.2</b> Linear algebra</a>
<ul>
<li class="chapter" data-level="A.2.1" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#eigenvalues-eigenvectors-and-eigendecomposition-of-a-matrix"><i class="fa fa-check"></i><b>A.2.1</b> Eigenvalues, eigenvectors and eigendecomposition of a matrix</a></li>
<li class="chapter" data-level="A.2.2" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#singular-value-decomposition"><i class="fa fa-check"></i><b>A.2.2</b> Singular value decomposition</a></li>
<li class="chapter" data-level="A.2.3" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#singular-vectors-for-data-representation"><i class="fa fa-check"></i><b>A.2.3</b> Singular vectors for data representation</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#useful-theorems-around-linear-algebra"><i class="fa fa-check"></i><b>A.3</b> Useful theorems around linear algebra</a></li>
<li class="chapter" data-level="A.4" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#inequalities"><i class="fa fa-check"></i><b>A.4</b> Inequalities</a>
<ul>
<li class="chapter" data-level="A.4.1" data-path="math-and-linear-algebra.html"><a href="math-and-linear-algebra.html#trigonometry"><i class="fa fa-check"></i><b>A.4.1</b> Trigonometry</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="series.html"><a href="series.html"><i class="fa fa-check"></i><b>B</b> Series</a></li>
<li class="chapter" data-level="C" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>C</b> Probability</a>
<ul>
<li class="chapter" data-level="C.1" data-path="probability.html"><a href="probability.html#measure-theory"><i class="fa fa-check"></i><b>C.1</b> Measure theory</a>
<ul>
<li class="chapter" data-level="C.1.1" data-path="probability.html"><a href="probability.html#boosting-probabilities-with-median-lemma-or-powering-lemma"><i class="fa fa-check"></i><b>C.1.1</b> Boosting probabilities with “median lemma” (or powering lemma )</a></li>
</ul></li>
<li class="chapter" data-level="C.2" data-path="probability.html"><a href="probability.html#markov-chains"><i class="fa fa-check"></i><b>C.2</b> Markov chains</a></li>
<li class="chapter" data-level="C.3" data-path="probability.html"><a href="probability.html#distributions"><i class="fa fa-check"></i><b>C.3</b> Distributions</a></li>
<li class="chapter" data-level="C.4" data-path="probability.html"><a href="probability.html#concentration-inequalities"><i class="fa fa-check"></i><b>C.4</b> Concentration inequalities</a>
<ul>
<li class="chapter" data-level="C.4.1" data-path="probability.html"><a href="probability.html#markov-inequality"><i class="fa fa-check"></i><b>C.4.1</b> Markov inequality</a></li>
<li class="chapter" data-level="C.4.2" data-path="probability.html"><a href="probability.html#chebyshev-inequality"><i class="fa fa-check"></i><b>C.4.2</b> Chebyshev inequality</a></li>
<li class="chapter" data-level="C.4.3" data-path="probability.html"><a href="probability.html#weak-law-of-large-numbers"><i class="fa fa-check"></i><b>C.4.3</b> (Weak) Law of large numbers</a></li>
<li class="chapter" data-level="C.4.4" data-path="probability.html"><a href="probability.html#chernoff-bound"><i class="fa fa-check"></i><b>C.4.4</b> Chernoff bound</a></li>
<li class="chapter" data-level="C.4.5" data-path="probability.html"><a href="probability.html#hoeffding-inequality"><i class="fa fa-check"></i><b>C.4.5</b> Hoeffding inequality</a></li>
</ul></li>
<li class="chapter" data-level="C.5" data-path="probability.html"><a href="probability.html#useful-quantum-subroutines-and-folklore-results"><i class="fa fa-check"></i><b>C.5</b> Useful quantum subroutines and folklore results</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="appendix-contributors.html"><a href="appendix-contributors.html"><i class="fa fa-check"></i><b>D</b> Contributions and acknowledgements</a>
<ul>
<li class="chapter" data-level="D.1" data-path="appendix-contributors.html"><a href="appendix-contributors.html#license-and-citation"><i class="fa fa-check"></i><b>D.1</b> License and citation</a></li>
<li class="chapter" data-level="D.2" data-path="appendix-contributors.html"><a href="appendix-contributors.html#cookie-policy"><i class="fa fa-check"></i><b>D.2</b> Cookie Policy</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>E</b> References</a></li>
<li class="divider"></li>
<li><a href="https://luongo.pro" target="blank">By Alessandro 'Scinawa' Luongo</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quantum algorithms for data analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="probability" class="section level1 hasAnchor" number="18">
<h1><span class="header-section-number">C</span> Probability<a href="probability.html#probability" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="measure-theory" class="section level2 hasAnchor" number="18.1">
<h2><span class="header-section-number">C.1</span> Measure theory<a href="probability.html#measure-theory" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div class="definition">
<p><span id="def:sigma-algebra" class="definition"><strong>Definition C.1  (Sigma algebra) </strong></span>Let <span class="math inline">\(\Omega\)</span> be a set, and <span class="math inline">\(\Sigma\)</span> be a subset of the power set of <span class="math inline">\(\Omega\)</span> (or equivalently a collection of subsets of <span class="math inline">\(X\)</span>). Then <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra if:</p>
<ul>
<li><span class="math inline">\(\emptyset\in \Sigma\)</span>,</li>
<li><span class="math inline">\(\Sigma\)</span> is closed under countable union,</li>
<li><span class="math inline">\(\forall S \in \Sigma, \overline{S} \in \Sigma\)</span>.</li>
</ul>
</div>
<p>Observe that thanks to de Morgan’s law, we can equivalently define the sigma algebra to be closed under countable intersection. Oftentimes, it’s common to conflate <span class="math inline">\(\Sigma\)</span> and <span class="math inline">\((\Omega, \Sigma)\)</span>, and call both <span class="math inline">\(\sigma\)</span>-algebra.</p>
<div class="definition">
<p><span id="def:measurable-space" class="definition"><strong>Definition C.2  (Measurable space) </strong></span>Let <span class="math inline">\(\Omega\)</span> be a set, and <span class="math inline">\(\Sigma\)</span> a <span class="math inline">\(\sigma\)</span>-algebra. The tuple <span class="math inline">\((\Omega, \Sigma)\)</span> is a measurable space (or Borel space).</p>
</div>
<div class="definition">
<p><span id="def:measurable-function" class="definition"><strong>Definition C.3  (Measurable function) </strong></span>Let <span class="math inline">\((\Omega, \Sigma)\)</span>, and <span class="math inline">\((Y, T)\)</span> two different measurable space. A function <span class="math inline">\(f : \Omega \mapsto Y\)</span> is said to be measurable if for every <span class="math inline">\(E \in T\)</span>:
<span class="math display">\[f^{-1}(E):=\{x \in \Omega | f(x) \in E \} \in \Sigma\]</span></p>
</div>
<p>A measurable function is a function between the underlying sets of two measurable spaces that preserves the structure of the spaces: the preimage of any measurable set is measurable. This is in <a href="https://en.wikipedia.org/wiki/Measurable_function">direct analogy</a> to the definition that a continuous function between topological spaces preserves the topological structure: the preimage of any open set is open.</p>
<!-- https://ece.iisc.ac.in/~parimal/2015/proofs/lecture-17.pdf -->
<div class="definition">
<p><span id="def:continious-function" class="definition"><strong>Definition C.4  (Continious function) </strong></span>Let <span class="math inline">\((X, \mathbb{X}), (Y, \mathbb{Y})\)</span> two topological spaces. Let <span class="math inline">\(f\)</span> be a function between two topological spaces <span class="math inline">\(f : X \mapsto Y\)</span> is said to be continious if the inverse image of every open subset of <span class="math inline">\(Y\)</span> is open in <span class="math inline">\(X\)</span>. In other words, if <span class="math inline">\(V \in \mathbb{Y}\)</span>, then its inverse image <span class="math inline">\(f^{-1}(V) \in \mathbb{X}\)</span></p>
</div>
<div class="definition">
<p><span id="def:measure-space" class="definition"><strong>Definition C.5  (Measure space) </strong></span>The tuple <span class="math inline">\((\Omega, \Sigma, \mathbb{P})\)</span> is a measure space if:</p>
<ul>
<li><span class="math inline">\((\Omega, \Sigma)\)</span> is a measurable space.</li>
<li><span class="math inline">\(\mu(E)\)</span> is a measure on <span class="math inline">\((\Omega, \Sigma)\)</span>:
<ul>
<li><span class="math inline">\(\mu : \Sigma \mapsto \mathbb{R}+\{-\infty, +\infty\}\)</span></li>
<li>non-negativity: <span class="math inline">\(\mu(E) \geq 0 \forall E \in \Sigma\)</span></li>
<li>Null empty set <span class="math inline">\(\mu(\emptyset )= 0\)</span></li>
<li>Coutable additivity (or <span class="math inline">\(\sigma\)</span>-additivity): for all countable collections <span class="math inline">\(\{E_k \}_{k=1}^\infty\)</span> of pariwise disjoint sets in <span class="math inline">\(\Sigma\)</span>,
<span class="math display">\[\mu \left(\cup_{k=1}^\infty E_k\right) = \sum_{k=1}^\infty \mu(E_k)\]</span></li>
</ul></li>
</ul>
</div>
<div class="definition">
<p><span id="def:probability-space" class="definition"><strong>Definition C.6  (Probability space) </strong></span>The tuple <span class="math inline">\((\Omega, \Sigma, \mathbb{P})\)</span> is a probability space if:</p>
<ul>
<li><span class="math inline">\((\Omega, \Sigma)\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra (<span class="math inline">\(\Omega\)</span> is the set of <em>outcomes</em> of the experiment, and <span class="math inline">\(\Sigma\)</span> is the set of <em>events</em>)</li>
<li><span class="math inline">\(\mathbb{P}\)</span> is a measurable function:
<ul>
<li><span class="math inline">\(\mathbb{P} : \Sigma \mapsto [0,1]\)</span></li>
<li>Null empty set.</li>
<li><span class="math inline">\(\mu\)</span> is countably additive.</li>
</ul></li>
<li><span class="math inline">\(\mathbb{P}(\Omega)=1\)</span></li>
</ul>
</div>
<p>I.e. a probability space is a measure space where the measurable function on <span class="math inline">\(\Omega\)</span> is <span class="math inline">\(1\)</span>.</p>
<div class="definition">
<p><span id="def:unnamed-chunk-101" class="definition"><strong>Definition C.7  (Complete probability space) </strong></span>For <span class="math inline">\(B \subset \Sigma\)</span> s.t. <span class="math inline">\(\mathbb{P}(B)=0\)</span>, a <span class="math inline">\((\Omega, \Sigma, \mathbb{P})\)</span> probability space is complete if <span class="math inline">\(\forall A \subset B\)</span>, <span class="math inline">\(A \in \Sigma\)</span>.</p>
</div>
<div class="definition">
<p><span id="def:equivalence-prob-measure" class="definition"><strong>Definition C.8  (Equivalence between probability measures) </strong></span>Let <span class="math inline">\((\Omega, \Sigma, \mathbb{P}), (\Omega, \Sigma, \mathbb{Q})\)</span> two probability space with the same <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(\Sigma\)</span>. We say that <span class="math inline">\(\mathbb{P}\)</span> and <span class="math inline">\(\mathbb{Q}\)</span> are equivalent iif for every <span class="math inline">\(A \in \Sigma\)</span>, <span class="math inline">\(\mathbb{P}(A)=0 \Leftrightarrow \mathbb{Q}(A)=0\)</span>.</p>
</div>
<p>It basically means that the two measures agree on the possible and impossible events (even if it is pretty strange to call them equivalent).</p>
<div class="definition">
<p><span id="def:random-variable" class="definition"><strong>Definition C.9  (Random variable) </strong></span>A (real-valued) random variable on a probability space <span class="math inline">\((\Omega, \Sigma, \mathbb{P})\)</span> is a measurable function <span class="math inline">\(X: \Omega \mapsto \mathbb{R}\)</span>.</p>
</div>
<!-- - Joint probability of two events $P(a \cap b) = P(a,b) = P(a|b|)P(a)$ -->
<!-- - Marginal probability $p(a) = \sum_b p(a,b) = \sum_b p(a|b)p(b)$ -->
<!-- - Union of two events $p(a \cup b)$ -->
<!-- - Sum rule -->
<!-- - Rule of total probability  -->
<!-- - Conditional probability -->
<!-- - Bayes' Theorem: $$ p(A=a | B=b) = \frac{p(A=a|B=b)}{p(B=b)} = \frac{p(A=a)p(B=b|A=a)}{\sum_{a'} p(A=a')p(B=b|A=a')}$$ -->
<p>Remember <a href="https://www.johnmyleswhite.com/notebook/2013/03/22/modes-medians-and-means-an-unifying-perspective/">that</a>, for a list of numbers <span class="math inline">\(x_1, x_n\)</span>,</p>
<ul>
<li>The mode can be defined as <span class="math inline">\(\arg\min_x \sum_i |x_i - x|^0\)</span></li>
<li>The median can be defined as <span class="math inline">\(\arg\min_x \sum_i |x_i - x|^1\)</span></li>
<li>The mean can be defined as <span class="math inline">\(\arg\min_x \sum_i |x_i - x|^2\)</span>.</li>
</ul>
<div id="union-bound" class="section level4 hasAnchor" number="18.1.0.1">
<h4><span class="header-section-number">C.1.0.1</span> Union bound<a href="probability.html#union-bound" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The union bound is used to show that the probability of union fo some finte or countable set of events is less than some value.</p>
<p><span id="unionbound"> </span></p>
<div class="theorem">
<p><span id="thm:unionbound" class="theorem"><strong>Theorem C.1  (Union bound) </strong></span><span class="math inline">\(\forall\)</span> events <span class="math inline">\(A_1 \dots A_n \in \Sigma\)</span>:
<span class="math display">\[P(\cup_{i=1}^n A_i) \leq \sum_{i=1}^n P(A_i)\]</span></p>
</div>
<!-- ```{proof} -->
<!-- It would be nice to have a proof by induction here, which is the standard proof for showing the union bound, based on the axioms of probability theory.  -->
<!-- ``` -->
<div class="exercise">
<p><span id="exr:unnamed-chunk-102" class="exercise"><strong>Exercise C.1  </strong></span>In Erdős–Rényi graphs <span class="math inline">\(G(n,p)\)</span>, (that is, a graph with <span class="math inline">\(n\)</span> nodes with probability <span class="math inline">\(p\)</span> that each of the two nodes are connected). We define the event <span class="math inline">\(B_n\)</span> as the event where a graph <span class="math inline">\(G(n,p)\)</span> has at least one isolated node. Show that <span class="math inline">\(P(B_n) \leq n(1-p)^{n-1}\)</span>.</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-45" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(A_i, i\in[n]\)</span> the event that node <span class="math inline">\(i\)</span> is isoldated. Its probability, from the definition of <span class="math inline">\(G(n,p)\)</span> is <span class="math inline">\((1-p)^{n-1}\)</span>, because there might be an edge with probability <span class="math inline">\(p\)</span> with other <span class="math inline">\(n-1\)</span> nods. From this, applying directly the union bonund we obtain an upper bound on the probability that there is at least one isoldated node is in the graph:
<span class="math display">\[P(B_n) = P(\cup_{i=1}^n A_i) \leq \sum_i P(A_i) \leq nP(A_i) = n(1-p)^{n-1}\]</span></p>
</div>
<div class="exercise">
<p><span id="exr:example-algorithm-unionbound" class="exercise"><strong>Exercise C.2  </strong></span>Suppose we run 4 times a randomized algorithm, with success probability <span class="math inline">\(1-\delta\)</span>. Can you bound the probability that we never fail using the union bound?</p>
</div>
<div class="proof">
<p><span id="unlabeled-div-46" class="proof"><em>Proof</em>. </span>Let <span class="math inline">\(f_i\)</span> the event that we fail running our algorithm at time <span class="math inline">\(i\)</span>.
We know that the failure probability <span class="math inline">\(f_i\)</span> is <span class="math inline">\(\delta\)</span> for all <span class="math inline">\(i \in [4]\)</span>. Thanks to the union bound we can bound the probability that we fail at least once:
<span class="math inline">\(P(\cup_i^k f_i ) \leq \sum_i^4 \delta = 4\delta\)</span>.
It follows that the have 4 success in a row is <em>lower</em> bounded by <span class="math inline">\(1-4\delta\)</span>.</p>
<p>Note that we could have also bypassed the union bound and compute this quantity analitically, as the probability of getting 4 success in a row would be <span class="math inline">\((1-\delta)^4\)</span>, which we can compute with the binomial theorem <a href="math-and-linear-algebra.html#thm:binomial-theorem">A.2</a>.</p>
</div>
<!-- Maybe we can be more precise in this exercise, by saying the interpretation of the values of difference between the analytic formula and the bound. Also we should say that the union bound might give "probabilities" bigger than one (i.e. if $\delta < 1/4$ in this case). Overall, I'm not satisfied with the level of clarity of this example, but i think it's nice to have. -->
<p><span id="variance"> </span>
::: {.definition #variance name=“Variance”}
<span class="math display">\[\begin{align}
\operatorname{Var}(X) &amp;= \operatorname{E}\left[(X - \operatorname{E}[X])^2\right] \\[4pt]
&amp;= \operatorname{E}\left[X^2 - 2X\operatorname{E}[X] + \operatorname{E}[X]^2\right] \\[4pt]
&amp;= \operatorname{E}\left[X^2\right] - 2\operatorname{E}[X]\operatorname{E}[X] + \operatorname{E}[X]^2 \\[4pt]
&amp;= \operatorname{E}\left[X^2 \right] - \operatorname{E}[X]^2
\end{align}\]</span>
:::</p>
<div class="exercise">
<p><span id="exr:unnamed-chunk-105" class="exercise"><strong>Exercise C.3  </strong></span>How can we express the variance as expectation of quantum states? What quantum algorithm might we run to estimate the variance of a random variable <span class="math inline">\(M\)</span>?
<span class="math display">\[\langle\psi|M^2|\psi\rangle - (\langle\psi|M|\psi\rangle)^2 \]</span>
Discuss.</p>
</div>
<div class="definition">
<p><span id="def:expofamily" class="definition"><strong>Definition C.10  (Exponential Family [@murphy2012machine]) </strong></span>A probability density function or probability mass function <span class="math inline">\(p(v|\nu)\)</span> for <span class="math inline">\(v = (v_1, \cdots, v_m) \in \mathcal{V}^m\)</span>, where <span class="math inline">\(\mathcal{V} \subseteq \mathbb{R}\)</span>, is a <span class="math inline">\(\sigma\)</span>-algebra over a set <span class="math inline">\(X\)</span> <span class="math inline">\(\nu \in \mathbb{R}^p\)</span> is said to be in the exponential family if it can be written as:
<span class="math display">\[p(v|\nu) :=  h(v)\exp \{ o(\nu)^TT(v) - A(\nu) \}\]</span>
where:</p>
<ul>
<li><span class="math inline">\(\nu \in \mathbb{R}^p\)</span> is called the parameter of the family,</li>
<li><span class="math inline">\(o(\nu)\)</span> is a function of <span class="math inline">\(\nu\)</span> (which often is just the identity function),</li>
<li><span class="math inline">\(T(v)\)</span> is the vector of sufficient statistics: a function that holds all the information the data <span class="math inline">\(v\)</span> holds with respect to the unknown parameters,</li>
<li><span class="math inline">\(A(\nu)\)</span> is the cumulant generating function, or log-partition function, which acts as a normalization factor,</li>
<li><span class="math inline">\(h(v) &gt; 0\)</span> is the which is a non-informative prior and de-facto is scaling constant.</li>
</ul>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level4 hasAnchor" number="18.1.0.2">
<h4><span class="header-section-number">C.1.0.2</span> Bias-variance tradeoff<a href="probability.html#bias-variance-tradeoff" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><a href="http://fourier.eng.hmc.edu/e176/lectures/probability/node9.html">Here</a> is a nice reference to understand the bias-variance tradeoff</p>
</div>
<div id="boosting-probabilities-with-median-lemma-or-powering-lemma" class="section level3 hasAnchor" number="18.1.1">
<h3><span class="header-section-number">C.1.1</span> Boosting probabilities with “median lemma” (or powering lemma )<a href="probability.html#boosting-probabilities-with-median-lemma-or-powering-lemma" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section we discuss the following, widely known result in CS. It’s used not only in writing algorithms, but also in complexity theory.</p>
<div class="lemma">
<p><span id="lem:powering-lemma" class="lemma"><strong>Lemma C.1  (Powering lemma [@jerrum1986random]) </strong></span>Let <span class="math inline">\(\mathcal{A}\)</span> be a quantum or classical algorithm which aims to estimate some quantity <span class="math inline">\(\mu\)</span>, and whose output <span class="math inline">\(\widetilde{\mu}\)</span> satisfies <span class="math inline">\(|\mu-\widetilde{\mu} |\leq \epsilon\)</span> except with probability <span class="math inline">\(\gamma\)</span>, for some fixed <span class="math inline">\(\gamma \leq 1/2\)</span>. Then, for any <span class="math inline">\(\gamma &gt; 0\)</span> it suffices to repeat <span class="math inline">\(\mathcal{A}\)</span> <span class="math inline">\(O(\log 1/\delta)\)</span> times and take the median to obtain an estimate which is accurate to within <span class="math inline">\(\epsilon\)</span> with probability at least <span class="math inline">\(1-\delta\)</span>.</p>
</div>
<!-- Suppose the following Lemma: -->
<!-- ```{theorem} -->
<!-- There exist an algorithm $A$ output $\overline{\mu}$ that estimates $\mu$ with probability $1/16$ with error $\epsilon m$. That is: -->
<!-- $$P[ |\mu - \overline{\mu} | \leq m \epsilon ] \geq \frac{1}{16}  $$ -->
<!-- ``` -->
<!-- Using the median trick, we can boost the probablity of success like this: -->
<!-- ```{theorem, name="[@betterupperboundSDP"} -->
<!-- There exist an algorithm $A'$ that estimates $\mu$ with probability $1-\delta$ with error $\epsilon m$. It is obtained by repeating $O(\log(1/\delta)$ the algorithm $A$.  -->
<!-- ``` -->
<!-- ``` -->
<!-- Let's pick the median of $K=O(\log(1/\delta)$ repetitions. Let's $F_i$ for $i \in [K]$ the output of previous algorithm. Let $z_K$ denote the median.  -->
<!-- $$Pr(|z_k - \mu| \geq \epsilon m) = Pr(z_k - \geq \epsilon m + \mu ) + Pr(z_k  \leq  \mu - \epsilon m )$$ -->
<!-- We can upper bound the first term as: \textcolor{red}{detail better why first passage} -->
<!-- \begin{align*} -->
<!--   (*) &\leq  \sum_{I \subseteq [K]: |I| \geq K/2} \prod_{i \in I} \mathrm{Pr}\big( (F)_i \geq \epsilon m + \mu \big) \\ -->
<!--       &\leq  (|\{I \subseteq [K]: |I| \geq K/2 \}| ) \left(\frac{1}{16}\right)^{K/2} \\ -->
<!--       &=  2^{K-1}\left(\frac{1}{4}\right)^K \\ -->
<!--       &\leq \frac{1}{2}\textcolor{red}{\left(\frac{1}{2}\right)^{\log_2(1/\delta)}} = \frac{1}{2} \delta. -->
<!-- \end{align*} -->
<!-- Analogously, one can show that $Pr\big( z_K \leq   - \epsilon m  + \mu \big) \leq \frac{1}{2} \delta$. Hence -->
<!-- \[\mathrm{Pr}\big(|z_K - \mu| \geq \epsilon m\big) \leq \delta. \] -->
<!-- ``` -->
</div>
</div>
<div id="markov-chains" class="section level2 hasAnchor" number="18.2">
<h2><span class="header-section-number">C.2</span> Markov chains<a href="probability.html#markov-chains" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Useful resources: <a href="https://www.probabilitycourse.com/chapter11/11_2_4_classification_of_states.php#:~:text=A%20Markov%20chain%20is%20said%20to%20be%20irreducible%20if%20all,always%20stay%20in%20that%20class.">here</a>, <a href="http://www.columbia.edu/~ww2040/4701Sum07/4701-06-Notes-MCII.pdf">here</a>.</p>
<div class="definition">
<p><span id="def:markov-chain" class="definition"><strong>Definition C.11  (Markov chain [@serfozo2009basics]) </strong></span>Let <span class="math inline">\((X_t)_{t \in I}\)</span> be a stochastic process defined over a probability space <span class="math inline">\((\Omega, \Sigma, \mathbb{P})\)</span>, for a countable set <span class="math inline">\(I\)</span>, where <span class="math inline">\(X_t\)</span> are random variables on a set <span class="math inline">\(\mathcal{S}\)</span> (called state space). Then <span class="math inline">\((X_t)_{t \in I}\)</span> is a Markov chain if, for any <span class="math inline">\(j \in \mathcal{S}\)</span> and <span class="math inline">\(t \geq 0\)</span>, it holds that
<span class="math display">\[\mathbb{P}[X_{t+1} = j | X_0, X_1, \dots X_t] = \mathbb{P}[X_{t+1} =j | X_t]\]</span>
and for all <span class="math inline">\(j,i \in \mathcal{S}\)</span>, it holds that
<span class="math display">\[\mathbb{P}[X_{t+1} = j | X_t = i] = p_{ij}\]</span>,
where <span class="math inline">\(p_{ij}\)</span> is the transition probability for the Markov chain to go from state <span class="math inline">\(i\)</span> to state <span class="math inline">\(j\)</span>.</p>
</div>
<p>Less formally, a Markov chain is a stochastic process with the Markov property, for which we can just use a matrix <span class="math inline">\(P\)</span> to identify its transition probability. Most of the time, we will discretize the state space <span class="math inline">\(\mathcal{S}\)</span>, so we can label elements of <span class="math inline">\(\mathcal{S}\)</span> with integers <span class="math inline">\(i \in [|\mathcal{S}|]\)</span>. This fact will allow us to conflate the (push-forward) measure <span class="math inline">\(\mathcal{P}\)</span> on <span class="math inline">\(\mathcal{S}\)</span> and the matrix <span class="math inline">\(P\)</span>.</p>
<p>A state <span class="math inline">\(j\)</span> is said to be <em>accessible</em> from <span class="math inline">\(i\)</span> (written as <span class="math inline">\(i \mapsto j\)</span>) if <span class="math inline">\(P_{ij}^t &gt; 0\)</span> for some <span class="math inline">\(t\)</span>, where <span class="math inline">\(P^t\)</span> is the <span class="math inline">\(t\)</span>-th power of the transition matrix <span class="math inline">\(P\)</span>. A communication class is an equivalence releation between states (relatively simple to prove) where two states <span class="math inline">\(j,i\)</span> are said to communicate if they are mutually accessible.</p>
<div class="definition">
<p><span id="def:irreducible-markov-chain" class="definition"><strong>Definition C.12  (Irreducible markov chain [@]) </strong></span>A Markov Chain <span class="math inline">\((X_t)_{t \in I}\)</span> is irreducible if and only if</p>
<ul>
<li>there exist some integer <span class="math inline">\(t \in I\)</span> such that <span class="math inline">\(p^t_{ij} &gt; 0\)</span> for all <span class="math inline">\(i,j \in \mathcal{S}\)</span>
there exist some integer <span class="math inline">\(t \in I\)</span> such that <span class="math inline">\(P[X_t =j| X_0 = i] &gt; 0\)</span>, for all <span class="math inline">\(i,j \in \mathcal{S}\)</span></li>
<li>there is only one communication class.</li>
</ul>
<p>The previous conditions are equivalent.</p>
</div>
<p>In terms of random walks, irreducibility means that: if the graph is undirected, the graph is has only one connected component (i.e. is connected), and if the graph is directed, the graph is strongly connected.</p>
</div>
<div id="distributions" class="section level2 hasAnchor" number="18.3">
<h2><span class="header-section-number">C.3</span> Distributions<a href="probability.html#distributions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- ### Binomial -->
<!-- The binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a ``yes and no'' question, and each with its own boolean-valued outcome: success/yes/true/one (with probability $p$) or failure/no/false/zero (with probability $q=1-p$). -->
<!-- The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one. -->
<!-- ### Geometric  -->
<!-- When is the geometric distribution an appropriate model?\cite{wikipedia-geometric} -->
<!-- \begin{itemize} -->
<!-- \item     The phenomenon being modeled is a sequence of independent trials. -->
<!-- \item     There are only two possible outcomes for each trial, often designated success or failure. -->
<!-- \item The probability of success, p, is the same for every trial. -->
<!--     \end{itemize} -->
<!-- ### and many others! -->
<p><a href="http://web.mit.edu/urban_or_book/www/book/chapter7/7.1.3.html">This</a> is a beautiful guide that shows you how to draw samples from a probability distribution.</p>
</div>
<div id="concentration-inequalities" class="section level2 hasAnchor" number="18.4">
<h2><span class="header-section-number">C.4</span> Concentration inequalities<a href="probability.html#concentration-inequalities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!--
# TODO Formalize better inequalities
# can these inequalities be formalized better? some examples are needed, and it would be nice to have different formulations, or something that gives intuition
# I would like to have a proper part on these concentration inequalities.
# This is very important in computer science and in algorithms analysiss because the runtimes and the errors are
# bounded using tools like these.
# labels: good first issue, help wanted
-->
<!--
THIS IS A NICE TUTORIAL!! Let's expand and integrate and formalize it better!
http://www.stat.rice.edu/~jrojo/PASI/lectures/TyronCMarticle.pdf -->
<div id="markov-inequality" class="section level3 hasAnchor" number="18.4.1">
<h3><span class="header-section-number">C.4.1</span> Markov inequality<a href="probability.html#markov-inequality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Markov inequality is an <em>upper bound for the probability that a non-negative function of a random variable</em>, that is greater than or equal to a positive constant. Especially in analysis, people refer to it as Chebyshev’s inequality (sometimes, calling it the <em>first</em> Chebyshev inequality, while referring to the <a href="https://en.wikipedia.org/wiki/Chebyshev%27s_inequality">“usual”</a> Chebyshev’s inequality as the second Chebyshev inequality or Bienaym{'e}’s inequality).</p>
<div class="theorem">
<p><span id="thm:markov" class="theorem"><strong>Theorem C.2  (Markov inequality) </strong></span>For all non-negative random variable, and <span class="math inline">\(a &gt; 0\)</span>, we have that:</p>
<ul>
<li><span class="math inline">\(Pr(X \geq a) \leq \frac{E[X]}{a}\)</span></li>
<li><span class="math inline">\(Pr(X \geq aE[X]) \leq \frac{1}{a}\)</span></li>
</ul>
</div>
<div class="proof">
<p><span id="unlabeled-div-47" class="proof"><em>Proof</em>. </span>Observe that :
<span class="math display">\[E[X] = P(X &lt; a) \cdot E[X|X&lt;a] +  P(X &gt; a) \cdot E[X|X&gt;a]\]</span>
As both of these expected values are bigger than zero, (using the nonnegativity hypothesis) we have that
<span class="math display">\[E[X] \geq P(X &gt; a) \dot E[X|X&gt;a] \]</span></p>
<p>Now is easy to observe that <span class="math inline">\(E[X|X&gt;a]\)</span> is at least <span class="math inline">\(a\)</span>, and by rearranging we obtain that:
<span class="math display">\[ \frac{E[X]}{a} \geq P(X &gt; a) \]</span></p>
<p>The second statement of the theorem follows from substitution, i.e. setting <span class="math inline">\(b=aE[X]\)</span> and using the previous statement.</p>
</div>
<div class="theorem">
<p><span id="thm:coro-markov" class="theorem"><strong>Theorem C.3  (Corollary of Markov inequality) </strong></span>Let <span class="math inline">\(f\)</span> be a monotone increasing (or noll) function on a space <span class="math inline">\(I\)</span>, and define the random variable on <span class="math inline">\(Y\)</span>.
<span class="math display">\[ P(Y \geq b) \leq \frac{E[f(Y)]}{f(b)} \]</span></p>
</div>
</div>
<div id="chebyshev-inequality" class="section level3 hasAnchor" number="18.4.2">
<h3><span class="header-section-number">C.4.2</span> Chebyshev inequality<a href="probability.html#chebyshev-inequality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This inequality tells us about the probability of finding the random variable <span class="math inline">\(X\)</span> away from the mean <span class="math inline">\(E[X]\)</span> is bounded by the variance of <span class="math inline">\(X\)</span>.</p>
<div class="theorem">
<p><span id="thm:chebyshev" class="theorem"><strong>Theorem C.4  (Chebyshev inequality) </strong></span>Let <span class="math inline">\(X\)</span> be a random variable with finite mean and variance, and <span class="math inline">\(\epsilon &gt; 0\)</span>. Then:
<span class="math display">\[Pr[|X - E[X]| \geq \epsilon]\leq \frac{\sigma^2}{\epsilon^2}\]</span></p>
<p>Moreover, if <span class="math inline">\(k = \epsilon /\sigma\)</span> we can replace <span class="math inline">\(\epsilon\)</span> with <span class="math inline">\(k\sigma\)</span> and obtain:</p>
<p><span class="math display">\[Pr[|X - E[X]| \geq k\sigma]  \leq \frac{1}{k^2}\]</span></p>
</div>
<div class="proof">
  <p><span id = "unlabeled-div-48" class="proof"><em>Proof</em>.</span>Observe that <span class="math inline">\((X-E[X])^2\)</span> is a non-negative random variable. Therefore,
    <span class="math display">\[P(|X-E[X]| \geq \epsilon) = P((X-E[X])^2 \geq \epsilon^2)\]</span> Now since <span class="math inline">\((X-E[X])^2\)</span> is a non-negative
    random variable , we can apply Markov's inequality to get:
    <span class="math display">\[P(|X-E[X]| \geq \epsilon) = P((X-E[X])^2 \geq \epsilon^2) \leq \frac{E[(X-E[X])^2]}{\epsilon^2}\]</span>
    <span class="math display">\[P(|X-E[X]| \geq \epsilon) = P((X-E[X])^2 \geq \epsilon^2) \leq \frac{\sigma^2}{\epsilon^2}\]</span>
  </p>
</div>
<!-- # check if linearly smaller is correct/not confusing here-->
<p>It is very useful to see what happen when we define a new random variable <span class="math inline">\(Y\)</span> as the sample mean of <span class="math inline">\(X_1 \dots X_n\)</span> other random variables (iid) indipendent and identically distributed: <span class="math inline">\(Y= \frac{1}{n}\sum_i^n X_i\)</span>. The expected value of <span class="math inline">\(Y\)</span> is the same as the expected value of <span class="math inline">\(X\)</span>, but the variance is now linearly smaller in the number of samples:</p>
<p><span class="math display">\[E[Y] = \frac{1}{n} \sum_i^n E[X]\]</span>
<span class="math display">\[Var[Y] = \frac{1}{n^2} \sum_i^n \text{Var}[X]\]</span></p>
<p>This allows us to obtain the following bound:</p>
<div class="theorem">
<p><span id="thm:chebyshev-mean" class="theorem"><strong>Theorem C.5  (Chebyshev inequality for sample mean) </strong></span>Let <span class="math inline">\(Y= \frac{1}{n}\sum_i^n X_i\)</span>. Then,</p>
<p><span class="math display">\[Pr[|Y - E[Y]| \geq \epsilon]\leq \frac{\sigma^2}{n\epsilon^2}\]</span></p>
</div>
</div>
<div id="weak-law-of-large-numbers" class="section level3 hasAnchor" number="18.4.3">
<h3><span class="header-section-number">C.4.3</span> (Weak) Law of large numbers<a href="probability.html#weak-law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="theorem">
<p><span id="thm:wlln" class="theorem"><strong>Theorem C.6  ((Weak) Law of large numbers) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be i.i.d random variables with a finite expected value <span class="math inline">\(X[X_i]=\mu \leq \infty\)</span>. Then, for any <span class="math inline">\(\epsilon &gt; 0\)</span>, we have that:</p>
<p><span class="math display">\[\lim_{n\to +\infty} P\left( |\overline{X} - \mu | \geq \epsilon   \right) = 0\]</span>
  where <span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{i=1}^{n} X_i\)</span> is the sample mean.
</p>
</div>
<div class="proof">
  <p><span id = "unlabeled-div-49" class="proof"><em>Proof</em>.</span>Observe that <span class="math inline">\(E[\bar{X}] = \mu\)</span> and <span class="math inline">\(Var(\bar{X}) = \frac{\sigma^2}{n}\)</span>. By Chebyshev's Inequality,
    <span class="math diplay">\[P(|\bar{X} - \mu| &gt; \epsilon) \leq \frac{Var(\bar{X})}{\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \rightarrow 0 (as \hspace{0.2cm} n \rightarrow \infty)\]</span>
  </p>
</div>
</div>

<div id="strong-law-of-large-numbers" class="section level3 hasAnchor" number="18.4.4">
  <h3><span class="header-section-number">C.4.4</span> (Strong) Law of large numbers<a href="probability.html#weak-law-of-large-numbers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
  <div class="theorem">
  <p><span id="thm:wlln" class="theorem"><strong>Theorem C.7  ((Strong) Law of large numbers) </strong></span>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be i.i.d random variables with mean <span class="math-inline">\(\mu\)</span>.Let <span class="math-inline">\(\bar{X}=\frac{1}{n}\sum_i^n X_i\)</span>be the sample mean.
  Then,<span class="math inline">\(\bar{X}\)</span>converges almost surely to <span class="math inline">\(\mu\)</span>. That is, 
  <span class="math display">\[P(\lim_{n\to -\infty}\bar{X} - \mu) = 1\]</span>
  The SSLN implies WLLN but not-vicersa.
</p>
  </div>
  </div>
<div id="chernoff-bound" class="section level3 hasAnchor" number="18.4.5">
<h3><span class="header-section-number">C.4.5</span> Chernoff bound<a href="probability.html#chernoff-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<div id="chernoff-bound" class="section level3 hasAnchor" number="18.4.4">
<h3><span class="header-section-number">C.4.4</span> Chernoff bound<a href="probability.html#chernoff-bound" class="anchor-section" aria-label="Anchor link to header"></a></h3>

<!-- From [here](https://math.mit.edu/~goemans/18310S15/chernoff-notes.pdf) and [here](http://www.stat.cmu.edu/~arinaldo/Teaching/36709/S19/Scribed_Lectures/Jan29_Tudor.pdf). -->
<!-- https://polynomiallybounded.wordpress.com/2017/05/23/how-i-remember-the-chernoff-bound/ -->
<!-- We now consider a more restricted class of random variables, i.e. the case when our random variable is obtained as the sum of *indipendent* other random variables.  -->
<!-- Central limit theorem says that, as $n \to \infty$, the value $\frac{X-\mu}{\sigma}$ approaches the standard normal distribution $N(0,1)$. Hoewver, it does not tell any information on the rate of convergence. -->
<!-- <!-- (Expand more on this..) -->
<!-- Chernoff bound is useful because some random variable are not efficiently bounded from Markov and Chebyshev.  -->
<!-- Note: to apply the Chernoff bound we require our random variable to be a sum of *indipendent* random variables.  -->
<!-- https://www.probabilitycourse.com/chapter6/6_2_3_chernoff_bounds.php -->
<div class="theorem">

<p><span id="thm:chernoff-bound" class="theorem"><strong>Theorem C.8  (Chernoff bound) </strong></span>Let <span class="math inline">\(X=\sum_i^n X_i\)</span> where <span class="math inline">\(X_i =1\)</span> with probability <span class="math inline">\(p_i\)</span> and <span class="math inline">\(X_i=0\)</span> with probability <span class="math inline">\((1-p_i)\)</span>, and all <span class="math inline">\(X_i\)</span> are independent. Let <span class="math inline">\(\mu=E[X] = \sum_i^n p_i\)</span>. Then:</p>

<ul>
<li><em>Upper tail</em>: <span class="math inline">\(P(X \geq (1+\delta)\mu) \leq e^-\frac{\delta^2}{2+\delta}\mu\)</span> for all <span class="math inline">\(\delta &gt; 0\)</span></li>
<li><em>Lower tail</em>: <span class="math inline">\(P(X \leq (1-\delta)\mu) \leq e^{\mu\delta^2/2}\)</span> for all <span class="math inline">\(0 \leq \delta \leq 1\)</span></li>
</ul>
</div>
<div class="theorem">

<p><span id="thm:chernoff-bound2" class="theorem"><strong>Theorem C.9  (Chernoff bound) </strong></span>Suppose <span class="math inline">\(X_1, \dots, X_t\)</span> are independent random variables taking values in <span class="math inline">\(\{0,1\}\)</span>. Let <span class="math inline">\(M_t= (X_1 + \dots X_t)/t\)</span> denote their average value. Then for any <span class="math inline">\(0 &lt; \epsilon &lt; 1\)</span>,</p>
<ul>
<li>(Multiplicative) <span class="math inline">\(Pr[M_t - \mu \leq -\epsilon \mu] \leq exp^{-\frac{t\mu\epsilon^2}{2}}\)</span> and <span class="math inline">\(Pr[M_t - \mu \geq \epsilon \mu] \leq exp^{-\frac{t\mu\epsilon^2}{3}}\)</span></li>
<li>(Additive) <span class="math inline">\(Pr[M_t - \mu \leq -\epsilon ] \leq exp^{-2t\epsilon^2}\)</span> and <span class="math inline">\(Pr[M_t - \mu \geq \epsilon ] \leq exp^{-2t\epsilon^2}\)</span></li>
</ul>
</div>
<!-- ```{theorem, chernoff-bound-multiplicative, name="Chernoff bound multiplicative form"} -->
<!-- ``` -->
<!-- *Trick:* if not all random variables are between $0$ and $1$, we can define $Y_i=X_i/max(X_i)$ -->
<!-- <!-- (Expand more on this..) -->
<!-- # ```{exercise} -->
<!-- # estimate number of
<!-- # ``` -->
</div>
<div id="hoeffding-inequality" class="section level3 hasAnchor" number="18.4.5">
<h3><span class="header-section-number">C.4.5</span> Hoeffding inequality<a href="probability.html#hoeffding-inequality" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="lemma">
<p><span id="lem:Hoeffding" class="lemma"><strong>Lemma C.2  (Hoeffding inequality) </strong></span>Let <span class="math inline">\(X_1,\ldots,X_k\)</span> be independent random variables bounded by the interval <span class="math inline">\([a, b]\)</span>. Define the empirical mean of these variables by <span class="math inline">\(\overline{X} = \frac{1}{k} (X_1+\cdots+X_k)\)</span>, then
<span class="math display">\[Pr(|\overline{X} - \mathbb{E}[X]|\leq \epsilon) \geq 1-2
    \exp\left(-\frac{2k\epsilon^2}{b-a} \right).\]</span>
Consequently, if <span class="math inline">\(k\geq (b-a)\epsilon^{-2}\log(2/\eta)\)</span>, then
<span class="math inline">\(\overline{X}\)</span> provides an <span class="math inline">\(\epsilon\)</span>-approximation of <span class="math inline">\(\mathbb{E}[X]\)</span> with probability at least <span class="math inline">\(1-\eta\)</span>.</p>
</div>
<!-- # Polynomial approximations of useful functions -->
<!-- ```{lemma, poly-approx-ln, name="Polynomial approximations of logarithm [@distributional]"} -->
<!--        Let $\beta\in(0,1]$, $\eta\in(0,\frac{1}{2}]$ and $t\geq 1$. There exists a polynomial $\tilde{S}$ such that -->
<!--        $\forall x\in [\beta,1]$, $|\tilde{S}(x)-\frac{\ln(1/x)}{2\ln(2/\beta)}|\leq\eta$,  and $\,\forall x\in[-1,1]\colon -1\leq\tilde{S}(x)=\tilde{S}(-x)\leq 1$. Moreover $\text{deg}(\tilde{S})=O({\frac{1}{\beta}\log (\frac{1}{\eta} )})$. -->
<!-- ``` -->
<!-- <!-- -->
<!-- # TODO Add explaination on how to do the polynomial approximation of 1/x -->
<!-- # It mightbe cool to have the polynomial approximatoin used for ridge regression. Can we do it? -->
<!-- # labels: help wanted, enhancement -->
<!-- -->
<p>–&gt;</p>
<!-- # Error propagation and approximation {#error-prop} -->
<!-- This part is based on many different sources, like [@hogan2006combine], [@ku1966notes]. -->
<!-- ```{definition, absolute-error, name="Absolute error"} -->
<!-- $$|A - \overline{A} | = \epsilon_{Abs}$$ -->
<!-- ``` -->
<!-- ```{definition, relative-error, name="Relative error"} -->
<!-- $$\frac{| A - \overline{A}| }{A}  = \epsilon_R \text{ or equivalently}$$ -->
<!-- $$ A(1-\epsilon_{R}) \leq  \overline{A} \leq A(1+\epsilon_{R})$$ -->
<!-- ``` -->
<!-- Thus observe that: -->
<!-- - If (and only if) $|A| < 1$, then, $\epsilon_{Abs} \leq \epsilon_{R}$ -->
<!-- - If (and only if) $|A| > 1$, then, $\epsilon_{Abs} \geq \epsilon_{R}$ -->
<!-- We will study the relation between the two errors, often leveraging the idea of seeing what happen when we set $\epsilon_{Abs} = \epsilon_R A$. -->
<!-- Let's imagine an algorithm $\mathcal{A}$ that estimates a quantity $A$ in time $O(\text{poly}(\epsilon^{-1}))$. We would like to move from a relative to absolute precision, or vice versa. -->
<!-- ##### From absolute to relative precision -->
<!-- Suppose that we have an algorithm that in time $O(f(\frac{1}{\epsilon_{Abs}}))$ gives us $|A-\overline{A}  | \leq \epsilon_{Abs}$ for $\epsilon_{Abs} \in (0, 1]$ and we want a relative error $\epsilon_R > 0$: -->
<!-- - If $|A| < 1$, then we need to set $\epsilon_{Abs} = \epsilon_R A$. For this, we need to have a lower bound $\lambda^{-1}$ on $A$. If we have it, we can just set $\epsilon_{Abs} = \epsilon_R \lambda^{-1}$ and run our algorithm in time  $O(f(\frac{\lambda}{\epsilon_{Abs}}))$ -->
<!-- - If $|A| > 1$, If we want a relative error $\epsilon_R$, then by running the algorithm with $\epsilon_{Abs}=\epsilon_R$ we have already a relative error bound, as $\frac{|A- \overline{A}|}{A} \leq |A- \overline{A}| \leq \epsilon_{Abs}$Note that we $\epsilon_{Abs}$ is meant to stay $\in (0,1]$ as it wouldn't make sense to have a runtime of $O(\frac{1}{\epsilon_{abs}})$ for $\epsilon_{abs} > 1$. -->
<!-- <!-- TODO  -->
<p>–&gt;
<!-- <!-- - Are there cases of algorithms with $\epsilon_{abs} > 1$? --> –&gt;
<!-- <!-- - find examples! --> –&gt;</p>
<!-- ##### From relative to absolute precision -->
<!-- If we have an algorithm that in time $O(f(\frac{1}{\epsilon_{R}}))$ gives us $|A-\overline{A}  | \leq A\epsilon_{R}$ and we want an absolute $\epsilon_{Abs}$: -->
<!-- <!-- - IF $A \leq 1$, $$|A- \overline{A} | \leq \epsilon_R A \Rightarrow |A- \overline{A} | \leq \epsilon_{R} $$ because absolute error is an upper bound of the relative error. So if we use an algorithm to get a relative error estimate $\epsilon_R$ we automatically have an absolute error estimate $\epsilon_{Abs} = \epsilon_R$. If we have a lower bound $\lambda$ on $A$, than we can set the error of the algorithm $\epsilon_R = \epsilon_{Abs}$, to get the absolute precision we require. In case we have a lower bound on $A$, and we don't need a significantly precise estimate of $A$, we can speedup the algorithm by requiring  less ``dependence'' on $\epsilon_R$. We want to find $\epsilon_R' \leq \epsilon_{Abs}/A$. As we don't have an estimate for $A$ yet, we need a lower bound for $A$.  \textcolor{red}{check if true that is lower!} -->
<p>–&gt;</p>
<!-- <!--         \emph{Example:} Amplitude estimation output a scalar $0 \leq \widetilde{p}\leq 1$ which equal some probability $p$, such that $|p-\widetilde{p}| \leq \epsilon p$ in time $O \left(\frac{1}{\epsilon p}\right)$. We have directly an absolute error of $\epsilon$ in this estimate (which we will rarely use, as we like to multiply this estimate ad make the error scale proportionately).  -->
<p>–&gt;</p>
<!-- <!-- -  IF $A > 1$,  -->
<p>–&gt;
<!-- <!--         $$|A- \overline{A} | \leq \epsilon_R A $$ --> –&gt;
<!-- <!--         we want   --> –&gt;
<!-- <!--          $$|A- \overline{A} | \leq \epsilon_{Abs} \text{by setting  } \epsilon_R = \frac{\epsilon_{Abs}}{\overline{A}}$$  --> –&gt;
<!-- <!--         By running algorithm $\mathcal{A}$ with error $\epsilon'=\frac{\epsilon}{A}$, i.e. we run it once with $\epsilon_R =1/4$ error, and than. --> –&gt;
<!-- <!--         We run it again with the improved $\epsilon_R=\frac{1}{\lambda}$, and we have a runtime of $O( \text{f}(\frac{A}{\epsilon^{-1}}))$. --> –&gt;</p>
<!-- <!--         \emph{Example:} We use an algorithm that gives the log-determinant (of a  matrix with spectral norm smaller than 1) with relative error $\frac{1}{4}$, so $\log\det(1-\frac{1}{4}) \leq \overline{\log\det} \leq \log\det(1+\frac{1}{4}) \Rightarrow \frac{3}{4}\leq \frac{\overline{\log\det}}{\log\det} \leq \frac{5}{4}$. Now we run it with error $\epsilon'_R =  \frac{\epsilon_{Abs}}{4\overline{\log\det}}$.  -->
<p>–&gt;
<!-- <!--         \textcolor{red}{maybe lo devo far correre anche all'inizio con $\epsilon=\epsilon_{abs}$ invece che 1/4?} --> –&gt;
<!-- <!--     \end{itemize} --> –&gt;</p>
<!-- <!-- \textbf{Exercies} -->
<p>–&gt;
<!-- <!-- This comes from \cite[trace estimation]{sanderthesis}\cite{van2020quantum} --> –&gt;
<!-- <!-- $$|A-B| \leq \epsilon_1 $$ --> –&gt;
<!-- <!-- $$|C-B| \leq \epsilon_2C $$ --> –&gt;
<!-- <!-- Can you prove that:  --> –&gt;
<!-- <!-- $$|C-A| \leq \epsilon_2(C+\epsilon_1) + \epsilon_1 $$ --> –&gt;</p>
<!-- ### Propagation of error in functions of one variable -->
<!-- <!-- - https://chem.libretexts.org/Bookshelves/Analytical_Chemistry/Supplemental_Modules_(Analytical_Chemistry)/Quantifying_Nature/Significant_Digits/Propagation_of_Error -->
<p>–&gt;
<!-- <!-- - https://foothill.edu/psme/daley/tutorials_files/10.%20Error%20Propagation.pdf --> –&gt;
<!-- <!-- - math.jacobs-university.de/oliver/teaching/jacobs/fall2015/esm106/handouts/error-propagation.pdf --> –&gt;
<!-- <!-- - and also \cite{hogan2006combine}. --> –&gt;</p>
<!-- <!-- In the following, $\Delta A$ is an absolute error.  -->
<p>–&gt;</p>
<!-- <!-- -  $A=\lambda B \Rightarrow \Delta A = |\lambda | \Delta B$ -->
<p>–&gt;
<!-- <!-- -  $A = \lambda/B \Rightarrow  \Delta A =  |\lambda /B^2| \Delta B = (substitute) = |A/B|\Delta B $  --> –&gt;
<!-- <!-- -  $A = \lambda B^\mu \Rightarrow \Delta A = |\mu \lambda B^{\mu^{-1}}|\Delta B = (substitute)= |\mu A /B | \Delta B$  --> –&gt;
<!-- <!-- -  $A = \lambda e^{\mu B} \Rightarrow \Delta A = |\mu A| \Delta B$ (where is $\lambda$?)  --> –&gt;
<!-- <!--     This because if  --> –&gt;
<!-- <!--     $$A = \lambda e^{\mu B}$$ --> –&gt;
<!-- <!--     The relative error is  --> –&gt;
<!-- <!--     $$\frac{\Delta A }{A} = \frac{\lambda \mu e^{ \mu B} \Delta B }{\lambda e^{\mu B}} = |\mu|\Delta B$$ --> –&gt;
<!-- <!--     so the absolute error follows from multiplying this quantity by $A$.  --> –&gt;</p>
<!-- <!-- -  $A = \kambda \ln (\mu B) \Rightarrow \Delta A = |\lambda/B|\Delta B$  -->
<p>–&gt;</p>
<!-- <!-- #### Propagation of error in function of more variables -->
<p>–&gt;</p>
<!-- <!-- ##### Linear combination of absolute error -->
<p>–&gt;
<!-- <!-- Imagine we have a derived quantity based on some measures: --> –&gt;
<!-- <!-- $$y = a+b+c $$ --> –&gt;</p>
<!-- <!-- We split this analysis in two cases: -->
<p>–&gt;</p>
<!-- <!-- If we don't have information on the nature of the error (i.e. we don't know the sign but only the magnitude. If $y = \sum_i (x_i + \epsilon_i)$, then  -->
<p>–&gt;
<!-- <!-- $$ \delta y = \delta a+\delta b+\delta c $$ --> –&gt;
<!-- <!-- $$ \delta y = \sum_i \left| \epsilon_i \right| = n\epsilon_{max} $$ --> –&gt;</p>
<!-- <!-- If we know that the error is Gaussian (i.e. maybe we have to prove something about mean and variance), and we can show than somehow they cancel each other, then: -->
<p>–&gt;</p>
<!-- <!-- $$\delta y=  \sqrt{\sum_i^n \epsilon_i^2} = \sqrt{n}\epsilon_{max}$$ -->
<p>–&gt;</p>
<!-- <!-- This often called the \emph{root mean squared error}. If these errors have a Gaussian distribution, than approx $68\%$ of the individual will lie between $y-\delta y$ and  $y+\delta y$ and $95\%$ of the individual will lie between $y-2\delta y$ and  $y+3\delta y$ -->
<p>–&gt;</p>
<!-- <!-- \subsection{Inverse of relative error \textcolor{blue}{[ok: - reread]} } -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq \epsilon A$ --> –&gt;
<!-- <!--     \end{itemize} --> –&gt;
<!-- <!--      $$|\frac{1}{A} - \frac{1}{\overline{A}}| =| \frac{A(1+\epsilon) - A}{A^2(1+\epsilon)} | = |\frac{A\epsilon}{A^2(1+\epsilon)}| = \frac{\epsilon}{A}\frac{1}{(1+\epsilon)} \leq  \frac{\epsilon}{A}   $$ --> –&gt;</p>
<!-- <!-- \subsection{Inverse of absolute error} -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq \epsilon $ --> –&gt;
<!-- <!--     \end{itemize} --> –&gt;</p>
<!-- <!--     $$| \frac{1}{A} - \frac{1}{\overline{A}}| = | \frac{\overline{A} - A}{A\overline{A}} |  = |\frac{A + \epsilon - A}{A\overline{A}}|  = |\frac{\epsilon}{A(A+\epsilon)}| = |\frac{\epsilon}{A}  \frac{1}{\overline{A}} | $$ -->
<p>–&gt;
<!-- <!--     \begin{itemize} --> –&gt;
<!-- <!--         \item     If $A > 1$. Then:  --> –&gt;
<!-- <!--         $$\leq | {\epsilon \over A\overline{A}}|  \leq |\frac{\epsilon}{A}| $$ --> –&gt;
<!-- <!--         \item If $A \leq 1$. Then: --> –&gt;
<!-- <!-- $$ \textcolor{red}{?} $$ --> –&gt;
<!-- <!--     \end{itemize} --> –&gt;</p>
<!-- <!-- \textcolor{red}{check passages because they give the same result of relative error} -->
<p>–&gt;</p>
<!-- <!-- \subsection{Relative error of product of relative errors \textcolor{blue}{[ok: - reread]}} -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq \epsilon_1 A$ --> –&gt;
<!-- <!-- \item $|B-\overline B | \leq \epsilon_2 B$ --> –&gt;
<!-- <!-- \end{itemize} --> –&gt;</p>
<!-- <!-- $$|AB - \overline{AB}| = |AB - AB(1+\epsilon_1)(1+\epsilon_2) | = $$ -->
<p>–&gt;
<!-- <!-- $$|AB - AB + AB\epsilon_1 + AB\epsilon_2 +AB\epsilon_1\epsilon_2 | = AB(\epsilon_1+\epsilon_2)  $$ --> –&gt;</p>
<!-- <!-- \subsection{Relative error of product of absolute error \textcolor{blue}{[ok: - reread]}} -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq \epsilon_1$ --> –&gt;
<!-- <!-- \item $|B-\overline B | \leq \epsilon_2$ --> –&gt;
<!-- <!-- \end{itemize} --> –&gt;</p>
<!-- <!-- $$\frac{|AB - \overline{AB}|}{AB} \leq ? $$  -->
<p>–&gt;
<!-- <!-- $$|AB- (A+\epsilon_1)(B+\epsilon_2)| = | AB - AB-A\epsilon_2 -B\epsilon_1 - \epsilon_1\epsilon_2| = |-A\epsilon_2 - B\epsilon_1 + O(\epsilon_1 \epsilon_2) | \Rightarrow$$ --> –&gt;
<!-- <!-- $$\frac{A\epsilon_2 + B\epsilon_1}{AB} = \frac{\epsilon_2}{B} + \frac{\epsilon_1}{A} $$  --> –&gt;</p>
<!-- <!-- \subsection{Ratio of absolute errors} -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq \epsilon_1$ --> –&gt;
<!-- <!-- \item $|B-\overline B | \leq \epsilon_2$ --> –&gt;
<!-- <!-- \end{itemize} --> –&gt;</p>
<!-- <!-- $$ \frac{A/B - \overline{A}/\overline{B} }{A/B} $$ -->
<p>–&gt;</p>
<!-- <!-- \subsection{Ratio of relative errors \textcolor{blue}{[ok: - reread]}} -->
<p>–&gt;
<!-- <!-- \begin{itemize} --> –&gt;
<!-- <!--     \item $|A-\overline A | \leq A\epsilon_1$ --> –&gt;
<!-- <!-- \item $|B-\overline B | \leq B\epsilon_2$ --> –&gt;
<!-- <!-- \end{itemize} --> –&gt;</p>
<!-- <!-- $$\frac{\overline{A}}{\overline{B}} = \frac{A(1+\epsilon_1)}{B(1+\epsilon_2)} = \frac{A}{B} (1+\epsilon_1) \frac{1}{(1+\epsilon_2)} $$ -->
<p>–&gt;</p>
<!-- <!-- We use Taylor approx of $\frac{1}{(1+\epsilon_2)} = 1-\epsilon_2 + \epsilon_2^2 $. Therefore,  -->
<p>–&gt;</p>
<!-- <!-- $$ \frac{\overline{A}}{\overline{B}} = \frac{A}{B}(1+\epsilon_1)(1-\epsilon_2)  $$  -->
<p>–&gt;
<!-- <!-- so --> –&gt;
<!-- <!-- $$|\frac{A}{B} - \frac{\overline{A}}{\overline{B}}|  = \frac{A}{B}|(\epsilon_1+\epsilon_2)|  \leq \frac{A}{B}(\epsilon_1+\epsilon_2) $$ --> –&gt;</p>
<!-- This is a better formalization and proof. -->
<!-- ```{theorem, ratio-relative-simple, name="[@sanderthesis] [@van2020quantum]"} -->
<!-- Let $0 \leq \theta \leq 1$ and let $\alpha, \beta, \tilde{\alpha}, \tilde{\beta}$ be positive real numbers, such that $|\alpha - \tilde{\alpha}| \leq \alpha\theta/3$, and  $|\beta - \tilde{\beta}| \leq \beta\theta/3$. Then: -->
<!-- $$\left|\frac{\alpha}{\beta} - \frac{\tilde{\alpha}}{\tilde{\beta}} \right| \leq \theta \frac{\alpha}{\beta}$$ -->
<!-- ``` -->
<!-- ```{proof} -->
<!-- $$\left|\frac{\alpha}{\beta} - \frac{\tilde{\alpha}}{\tilde{\beta}} \right|  = \left| \frac{\alpha\tilde{\beta}- \tilde{\alpha}\beta}{\beta\tilde{\beta}}  \right| = \left| \frac{\alpha\tilde{\beta}- \tilde{\alpha}\beta+\alpha\beta - \alpha\beta}{\beta\tilde{\beta}}  \right| =$$ -->
<!-- $$\left|\frac{\alpha\beta-\tilde{\alpha}\beta}{\beta\tilde{\beta}} \right| + \alpha\left| \frac{\beta -\tilde{\beta}}{\beta\tilde{\beta}} \right| \leq$$ -->
<!-- $$\left|\frac{\alpha -\tilde \alpha}{\tilde \beta} \right| + \frac{\alpha}{\tilde\beta}\theta/3$$ -->
<!-- Using the hypotesis on $|\alpha-\widetilde{\alpha}|\leq \alpha \theta/3$, -->
<!-- $$\leq \left|\frac{1}{\tilde{\beta}} \right|\frac{\alpha\theta}{3} + \frac{\alpha}{\tilde\beta}\theta/3$$ -->
<!-- Now we can use the fact that $\tilde \beta \geq \frac{2}{3}\beta$. This comes from the fact that if we have a relative error on $\beta$, we know that $|\tilde{\beta}| \leq \beta (1+\theta)$. In our case, $\beta(1-\frac{2}{3}\theta) \leq \tilde{\beta} \leq \theta (1+\frac{1}{3}\theta)$, so in the worst case, for $\theta=1$, we can simply see that $\tilde \beta \geq \frac{2}{3}\beta$. Now can put the lower bound on the denominator, and get: -->
<!-- $$\leq \left|\frac{\alpha\theta}{3\tilde{\beta}} \right| + \frac{\alpha}{\tilde\beta}\theta/3$$ -->
<!-- ``` -->
<!-- We can have a more general version of the bound. -->
<!-- ```{lemma, ratio-relative-general, name="[@hamoudi2020quantum]"} -->
<!-- Let $\tilde a$ be an estimate of $a>0$ such that $\vert \tilde a- a \vert \leq \epsilon_a a$. -->
<!-- with $\epsilon_a \in (0,1)$. -->
<!-- Similarly, let $\tilde b$ be an estimate of $b>0$ and $\epsilon_b \in (0,1)$ such that -->
<!-- $\vert \tilde b - b \vert \leq \epsilon_b b$. -->
<!-- Then the ratio $a/b$ is estimated to relative error -->
<!-- $\left \vert \frac{\tilde a}{\tilde b} -  \frac{a}{b}  \right\vert \leq \left ( \frac{\epsilon_a + \epsilon_b}{1-\epsilon_b} \right) \frac{a}{b}$. -->
<!-- ``` -->
<!-- The proof comes directly from their work -->
<!-- ```{proof} -->
<!-- Note that -->
<!-- $b -  \tilde{b} \leq \vert \tilde{b} - b\vert \leq \epsilon_b b$, so as we said before, -->
<!-- deduce $\frac{1}{ \tilde b} \leq \frac{1}{ b (1-\epsilon_b)}$. -->
<!-- Now we can combine the previous observation: -->
<!-- \begin{align} -->
<!-- \left| \frac{\tilde a}{\tilde b} -  \frac{a}{b}  \right| = & -->
<!--   \left \vert \frac{\tilde a b - a \tilde b}{\tilde b b}  \right\vert = \left \vert \frac{\tilde a b - ab + ab - a \tilde b}{\tilde b b}  \right\vert = \left \vert \frac{\tilde a  - a}{\tilde b} + \frac{a}{\tilde b} \frac{b - \tilde b}{ b}  \right\vert   \\ -->
<!-- \leq & \left \vert \frac{\tilde a  - a}{\tilde b} \right\vert+  \frac{a}{\tilde b}  \left \vert \frac{b - \tilde b}{ b}  \right\vert \leq \frac{\epsilon_a a + \epsilon_b a }{\tilde b}   \leq  \frac{a}{b}\frac{\epsilon_a +\epsilon_b}{ (1-\epsilon_b)}. -->
<!-- \end{align} -->
<!-- ``` -->
<!-- <!-- #### General case -->
<p>–&gt;</p>
<!-- <!-- ```{theorem, name="mean value theorem"} -->
<p>–&gt;
<!-- <!-- $$|f(x)- f(y)| \leq c |x-y|$$ --> –&gt;
<!-- <!-- ``` --> –&gt;</p>
<!-- <!-- - \url{https://math.stackexchange.com/questions/302177/proving-the-relative-error-of-division} -->
<p>–&gt;
<!-- <!-- - \url{https://math.stackexchange.com/questions/1153476/how-to-show-the-relative-error-of-fracx-ay-a?rq=1} --> –&gt;
<!-- <!-- - \url{https://math.stackexchange.com/questions/3000501/relative-error-of-division/3001050#3001050} --> –&gt;</p>
</div>
</div>
<div id="useful-quantum-subroutines-and-folklore-results" class="section level2 hasAnchor" number="18.5">
<h2><span class="header-section-number">C.5</span> Useful quantum subroutines and folklore results<a href="probability.html#useful-quantum-subroutines-and-folklore-results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We will often make use of a tool developed in <span class="citation">(<a href="#ref-wiebe2018quantum" role="doc-biblioref">Wiebe, Kapoor, and Svore 2018</a>)</span>. It is standard technique in classical computer science to boost the success probability of a randomized algorithm by repeating it and computing some statistics in the results. For the case of quantum algorithms, in high level, we take multiple copies of the output of the amplitude estimation procedure, compute the median, and reverse the circuit in order to get rid of the garbage.</p>
<div class="lemma">
<p><span id="lem:median" class="lemma"><strong>Lemma C.3  (Median evaluation [@wiebe2018quantum]) </strong></span>Let <span class="math inline">\(\mathcal{U}\)</span> be a unitary operation that maps
<span class="math display">\[\mathcal{U}:|0^{\otimes n}\rangle\mapsto \sqrt{a}|x,1\rangle+\sqrt{1-a} |G,0\rangle\]</span>
for some <span class="math inline">\(1/2 &lt; a \le 1\)</span> in time <span class="math inline">\(T\)</span>. Then there exists a quantum algorithm that, for any <span class="math inline">\(\Delta&gt;0\)</span> and for any <span class="math inline">\(1/2&lt;a_0 \le a\)</span>, produces a state <span class="math inline">\(|\Psi\rangle\)</span> such that <span class="math inline">\(\||\Psi\rangle-|0^{\otimes nL}\rangle|x\rangle\|\le \sqrt{2\Delta}\)</span> for some integer <span class="math inline">\(L\)</span>, in time
<span class="math display">\[
2T\left\lceil\frac{\ln(1/\Delta)}{2\left(|a_0|-\frac{1}{2} \right)^2}\right\rceil.
\]</span></p>
</div>
<p>We will report here some simple statements from literature which now are folklore.</p>
<div class="lemma">
<p><span id="lem:quattrocinque" class="lemma"><strong>Lemma C.4  ([@kerenidis2019qmeans]) </strong></span>Let <span class="math inline">\(\epsilon_b\)</span> be the error we commit in estimating <span class="math inline">\(|c\rangle\)</span> such that <span class="math inline">\(\left\lVert |c\rangle - |\overline{c}\rangle\right\rVert &lt; \epsilon_b\)</span>, and <span class="math inline">\(\epsilon_a\)</span> the error we commit in the estimating the norms, <span class="math inline">\(|\left\lVert c\right\rVert - \overline{\left\lVert c\right\rVert}| \leq \epsilon_a \left\lVert c\right\rVert\)</span>. Then <span class="math inline">\(\left\lVert\overline{c} - c\right\rVert \leq \sqrt{\eta} (\epsilon_a + \epsilon_b)\)</span>.</p>
</div>
<div class="lemma">
<p><span id="lem:kereclaim" class="lemma"><strong>Lemma C.5  ([@kerenidis2017quantumsquares]) </strong></span>Let <span class="math inline">\(\theta\)</span> be the angle between vectors <span class="math inline">\(x,y\)</span>, and assume that <span class="math inline">\(\theta &lt; \pi/2\)</span>.
Then, <span class="math inline">\(\left\lVert x-y\right\rVert \leq \epsilon\)</span> implies <span class="math inline">\(\left\lVert|x\rangle - |y\rangle\right\rVert \leq \frac{\sqrt{2}\epsilon}{\left\lVert x\right\rVert}\)</span>. Where <span class="math inline">\(|x\rangle\)</span> and <span class="math inline">\(|y\rangle\)</span> are two unit vectors in <span class="math inline">\(\ell_2\)</span> norm.</p>
</div>
</div>
</div>
<h3><span class="header-section-number">E</span> References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-wiebe2018quantum" class="csl-entry">
Wiebe, Nathan, Ashish Kapoor, and Krysta M Svore. 2018. <span>“Quantum Nearest-Neighbor Algorithms for Machine Learning.”</span> <em>Quantum Information and Computation</em> 15.
</div>
</div>
<script type="text/javascript" id="cookiebanner"
    src="https://cdn.jsdelivr.net/gh/dobarkod/cookie-banner@1.2.2/dist/cookiebanner.min.js"
    data-height="40px" data-position="bottom" data-bg="#d12224"
    data-message="This website uses cookies to enhance the browsing experience."
    data-moreinfo="https://quantumalgorithms.org/cookie-policy.html">
</script>
<!--gently powered by https://github.com/dobarkod/cookie-banner/ > -->
<a href="https://github.com/scinawa/quantumalgorithms.org" class="github-corner" aria-label="View source on GitHub"><svg width="80" height="80" viewBox="0 0 250 250" style="fill:#d12224; color:#fff; position: absolute; top: 0; border: 0; right: 0;" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>

<!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-W4MVX5C"
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->
            </section>

          </div>
        </div>
      </div>
<a href="series.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix-contributors.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["quantumalgorighms.pdf", "quantumalgorighms.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
